{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a9e6c",
   "metadata": {},
   "source": [
    "# Read Data From LM Eval Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a5ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json', 'top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json', 'top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json', 'top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json', 'top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json', 'top_evals/zaydzuhri__top-7B-4096-model/results_2025-08-20T10-46-34.638307.json', 'top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json', 'top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-20T14-15-37.501701.json', 'top_evals/zaydzuhri__mtp-7B-4096-model/results_2025-08-20T10-59-16.724946.json']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "eval_logs_folder = \"top_evals\"\n",
    "all_json_files = glob(f\"{eval_logs_folder}/*/*.json\")\n",
    "print(all_json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65811914",
   "metadata": {},
   "source": [
    "# Create Plot Using Background Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(json_sources, baseline_source_name, precision=2):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table where\n",
    "    each non-baseline cell is colored with a heatmap style based on its\n",
    "    difference from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for coloring.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Direct Cell Coloring ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Comparison of evaluation results. Colors relative to baseline.}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    column_format = \"l|l|\" + \"r\" * len(source_names)\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\", \"Metric\"] + source_names\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            # --- CORRECTED LOGIC FOR PERCENTAGE CONVERSION ---\n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val):\n",
    "                    return val\n",
    "                # Check if the metric is one of the exceptions.\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                # If it's NOT an exception, multiply by 100.\n",
    "                if not is_exception:\n",
    "                    return val * 100\n",
    "                return val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            \n",
    "            # Start the row with the uncolored baseline value\n",
    "            formatted_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            # Process each of the other sources for coloring\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    # For delta calculation, use original non-percentage values for better scaling\n",
    "                    original_baseline = row[baseline_source_name]\n",
    "                    original_compare = row.get(source)\n",
    "                    delta = (original_compare - original_baseline)\n",
    "                    \n",
    "                    # --- Heatmap Logic ---\n",
    "                    max_delta_for_color = 0.1 \n",
    "                    normalized_delta = min(abs(delta) / max_delta_for_color, 1.0)\n",
    "                    intensity = int(normalized_delta * 60)\n",
    "\n",
    "                    is_good = (delta < 0) if \"Perplexity\" in metric_name else (delta > 0)\n",
    "                    color = \"green\" if is_good else \"red\"\n",
    "                    \n",
    "                    cell_str = (f\"\\\\cellcolor{{{color}!{intensity}}}\"\n",
    "                                f\"{{{compare_val:.{precision}f}}}\")\n",
    "                    formatted_cells.append(cell_str)\n",
    "                else:\n",
    "                    formatted_cells.append('---')\n",
    "            \n",
    "            # Join all cells for the final row string\n",
    "            row_content = \" & \".join(formatted_cells)\n",
    "            if j == 0:\n",
    "                line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else:\n",
    "                line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76a7474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 26.54 & \\cellcolor{green!9}{28.07} & \\cellcolor{green!9}{28.07} \\\\\n",
      " & Acc Norm & 28.84 & \\cellcolor{green!6}{29.86} & \\cellcolor{green!3}{29.35} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 60.23 & \\cellcolor{green!21}{63.80} & \\cellcolor{green!18}{63.26} \\\\\n",
      " & Acc Norm & 56.52 & \\cellcolor{green!11}{58.38} & \\cellcolor{green!10}{58.29} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 35.52 & \\cellcolor{red!0}{35.38} & \\cellcolor{red!0}{35.43} \\\\\n",
      " & Acc Norm & 42.53 & \\cellcolor{green!1}{42.73} & \\cellcolor{green!6}{43.57} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 36.35 & \\cellcolor{red!6}{35.32} & \\cellcolor{green!4}{37.07} \\\\\n",
      " & Perplexity & 30.34 & \\cellcolor{red!60}{35.31} & \\cellcolor{green!60}{28.76} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 1.94 & \\cellcolor{green!2}{2.35} & \\cellcolor{green!1}{2.22} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 66.92 & \\cellcolor{red!3}{66.27} & \\cellcolor{green!3}{67.46} \\\\\n",
      " & Acc Norm & 66.65 & \\cellcolor{red!0}{66.49} & \\cellcolor{green!5}{67.57} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 83.20 & \\cellcolor{green!18}{86.30} & \\cellcolor{green!12}{85.30} \\\\\n",
      " & Acc Norm & 74.90 & \\cellcolor{green!15}{77.40} & \\cellcolor{green!29}{79.80} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & 39.82 & \\cellcolor{red!4}{39.00} & \\cellcolor{red!4}{39.00} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 4.93 & \\cellcolor{red!14}{2.55} & \\cellcolor{red!3}{4.37} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.86 & \\cellcolor{green!16}{0.88} & \\cellcolor{green!1}{0.86} \\\\\n",
      " & Byte Perplexity & 1.81 & \\cellcolor{red!20}{1.84} & \\cellcolor{red!1}{1.81} \\\\\n",
      " & Word Perplexity & 23.85 & \\cellcolor{red!60}{26.34} & \\cellcolor{red!60}{24.01} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "mtp_json_file = \"top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json\"\n",
    "vanilla_json_file = \"top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json\"\n",
    "top_json_file = \"top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json\"\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"TOP\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afde031a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "MTP    0.549709\n",
       "NTP    0.538886\n",
       "TOP    0.557166\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['metric'] == 'Acc Norm'].groupby('source')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8f9fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json', 'top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json', 'top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json', 'top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json', 'top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json', 'top_evals/zaydzuhri__top-7B-4096-model/results_2025-08-20T10-46-34.638307.json', 'top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json', 'top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-20T14-15-37.501701.json', 'top_evals/zaydzuhri__mtp-7B-4096-model/results_2025-08-20T10-59-16.724946.json']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "eval_logs_folder = \"top_evals\"\n",
    "all_json_files = glob(f\"{eval_logs_folder}/*/*.json\")\n",
    "print(all_json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b4fc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json\n",
      "top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json\n",
      "top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json\n"
     ]
    }
   ],
   "source": [
    "print(mtp_json_file)\n",
    "print(vanilla_json_file)\n",
    "print(top_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6ebe055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & Top \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 35.58 & \\cellcolor{green!16}{38.40} & \\cellcolor{green!22}{39.25} \\\\\n",
      " & Acc Norm & 38.65 & \\cellcolor{green!11}{40.61} & \\cellcolor{green!22}{42.32} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 72.81 & \\cellcolor{red!0}{72.69} & \\cellcolor{green!4}{73.48} \\\\\n",
      " & Acc Norm & 67.05 & \\cellcolor{green!21}{70.66} & \\cellcolor{green!18}{70.12} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 46.03 & \\cellcolor{red!8}{44.61} & \\cellcolor{red!1}{45.75} \\\\\n",
      " & Acc Norm & 60.05 & \\cellcolor{red!10}{58.29} & \\cellcolor{green!2}{60.45} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 49.58 & \\cellcolor{red!9}{47.93} & \\cellcolor{green!4}{50.34} \\\\\n",
      " & Perplexity & 11.38 & \\cellcolor{red!60}{13.69} & \\cellcolor{green!60}{11.19} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 4.54 & \\cellcolor{red!0}{4.46} & \\cellcolor{green!4}{5.37} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 73.78 & \\cellcolor{red!9}{72.20} & \\cellcolor{red!3}{73.23} \\\\\n",
      " & Acc Norm & 73.50 & \\cellcolor{red!2}{73.07} & \\cellcolor{green!3}{74.16} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 90.20 & \\cellcolor{green!3}{90.70} & \\cellcolor{green!4}{91.00} \\\\\n",
      " & Acc Norm & 86.40 & \\cellcolor{green!4}{87.20} & \\cellcolor{green!9}{87.90} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & 41.56 & \\cellcolor{green!3}{42.12} & \\cellcolor{green!5}{42.53} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 11.85 & \\cellcolor{green!24}{15.98} & \\cellcolor{green!42}{18.93} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.73 & \\cellcolor{green!19}{0.76} & \\cellcolor{red!0}{0.73} \\\\\n",
      " & Byte Perplexity & 1.66 & \\cellcolor{red!22}{1.70} & \\cellcolor{green!0}{1.66} \\\\\n",
      " & Word Perplexity & 15.09 & \\cellcolor{red!60}{17.03} & \\cellcolor{green!31}{15.04} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "size = \"1.8B\"\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    if size in json_file:\n",
    "        if \"mtp\" in json_file:\n",
    "            mtp_json_file = json_file\n",
    "        elif \"vanilla\" in json_file:\n",
    "            vanilla_json_file = json_file\n",
    "        elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "            top_json_file = json_file\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"Top\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b839ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source           task    metric     value\n",
      "1     NTP  Arc Challenge  Acc Norm  0.386519\n",
      "3     NTP       Arc Easy  Acc Norm  0.670455\n",
      "5     NTP      Hellaswag  Acc Norm  0.600478\n",
      "10    NTP           Piqa  Acc Norm  0.735038\n",
      "12    NTP           Sciq  Acc Norm  0.864000\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"metric\"] == \"Acc Norm\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b6c4b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "MTP    0.659666\n",
       "NTP    0.651298\n",
       "Top    0.669883\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['metric'] == 'Acc Norm'].groupby('source')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 45.05 & \\cellcolor{red!8}{43.69} & \\cellcolor{red!5}{44.20} \\\\\n",
      " & Acc Norm & 45.48 & \\cellcolor{green!0}{45.56} & \\cellcolor{green!5}{46.42} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 77.31 & \\cellcolor{green!2}{77.69} & \\cellcolor{green!4}{78.03} \\\\\n",
      " & Acc Norm & 74.07 & \\cellcolor{red!1}{73.86} & \\cellcolor{green!3}{74.62} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 50.95 & \\cellcolor{red!8}{49.58} & \\cellcolor{green!3}{51.53} \\\\\n",
      " & Acc Norm & 67.43 & \\cellcolor{red!9}{65.85} & \\cellcolor{green!7}{68.73} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 55.89 & \\cellcolor{red!16}{53.13} & \\cellcolor{green!6}{57.03} \\\\\n",
      " & Perplexity & 7.97 & \\cellcolor{red!60}{8.99} & \\cellcolor{green!60}{7.64} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 7.31 & \\cellcolor{green!0}{7.40} & \\cellcolor{green!2}{7.70} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 76.33 & \\cellcolor{red!7}{75.08} & \\cellcolor{red!0}{76.17} \\\\\n",
      " & Acc Norm & 77.04 & \\cellcolor{red!7}{75.73} & \\cellcolor{red!3}{76.39} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 92.90 & \\cellcolor{green!0}{93.00} & \\cellcolor{green!9}{94.50} \\\\\n",
      " & Acc Norm & 88.60 & \\cellcolor{green!4}{89.30} & \\cellcolor{green!18}{91.60} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & --- & --- & --- \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 24.28 & \\cellcolor{red!5}{23.36} & \\cellcolor{green!39}{30.90} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.66 & \\cellcolor{green!18}{0.69} & \\cellcolor{green!0}{0.66} \\\\\n",
      " & Byte Perplexity & 1.58 & \\cellcolor{red!20}{1.62} & \\cellcolor{red!0}{1.58} \\\\\n",
      " & Word Perplexity & 11.66 & \\cellcolor{red!60}{13.09} & \\cellcolor{red!1}{11.67} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "size = \"7B\"\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    if size in json_file:\n",
    "        if \"mtp\" in json_file:\n",
    "            mtp_json_file = json_file\n",
    "        elif \"vanilla\" in json_file:\n",
    "            vanilla_json_file = json_file\n",
    "        elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "            top_json_file = json_file\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"TOP\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51988e",
   "metadata": {},
   "source": [
    "# Create Plot Using Small Number at Bottom Corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "82333414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(\n",
    "        json_sources, \n",
    "        baseline_source_name, \n",
    "        precision=2, \n",
    "        size: str | None = None,\n",
    "        add_metric_column: bool = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table. Each\n",
    "    non-baseline cell contains the score and a small, colored delta indicating\n",
    "    the change from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for calculating the delta.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "        size (str | None): Optional size parameter to include in the caption.\n",
    "        add_metric_column (bool): If True, includes the 'Metric' column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "        pd.DataFrame: The processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Score and Small Delta ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\caption{{Comparison of evaluation results{' for size ' + size if size else ''}. Deltas relative to baseline.}}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    \n",
    "    column_format = \"l|\"\n",
    "    if add_metric_column:\n",
    "        column_format += \"l|\"\n",
    "    column_format += \"r\" * len(source_names)\n",
    "    \n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\"]\n",
    "    if add_metric_column:\n",
    "        header_cols.append(\"Metric\")\n",
    "    header_cols += source_names\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val): return val\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                return val * 100 if not is_exception else val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            value_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    delta = compare_val - baseline_val\n",
    "                    is_neutral = abs(delta) < 1e-6\n",
    "\n",
    "                    if is_neutral:\n",
    "                        cell_str = f\"{{{compare_val:.{precision}f}}}\"\n",
    "                    else:\n",
    "                        is_good = (delta > 0) if not any(ex.lower() in metric_name.lower() for ex in exception_to_percentage) else (delta < 0)\n",
    "                        color = \"green!70!black\" if is_good else \"red!70!black\"\n",
    "                        delta_part = f\"\\\\ \\\\textsubscript{{\\\\textcolor{{{color}}}{{{delta:+.{precision}f}}}}}\"\n",
    "                        cell_str = f\"{{{compare_val:.{precision}f}}}{delta_part}\"\n",
    "                    \n",
    "                    value_cells.append(cell_str)\n",
    "                else:\n",
    "                    value_cells.append('---')\n",
    "            \n",
    "            row_content = \" & \".join(value_cells)\n",
    "            \n",
    "            # --- FIXED LOGIC ---\n",
    "            if add_metric_column:\n",
    "                if j == 0: # First row of a task group\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "                else: # Subsequent rows\n",
    "                    line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else: # If not adding metric column\n",
    "                if j == 0:\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {row_content} \\\\\\\\\"\n",
    "                else:\n",
    "                    line = f\" & {row_content} \\\\\\\\\"\n",
    "            \n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "03f438bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results for size 7B. Deltas relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|rrr}\n",
      "\\toprule\n",
      "Task & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & 45.05 & {43.69}\\ \\textsubscript{\\textcolor{red!70!black}{-1.37}} & {44.20}\\ \\textsubscript{\\textcolor{red!70!black}{-0.85}} \\\\\n",
      " & 45.65 & {45.56}\\ \\textsubscript{\\textcolor{red!70!black}{-0.09}} & {46.42}\\ \\textsubscript{\\textcolor{green!70!black}{+0.77}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & 77.31 & {77.69}\\ \\textsubscript{\\textcolor{green!70!black}{+0.38}} & {78.03}\\ \\textsubscript{\\textcolor{green!70!black}{+0.72}} \\\\\n",
      " & 74.03 & {73.86}\\ \\textsubscript{\\textcolor{red!70!black}{-0.17}} & {74.62}\\ \\textsubscript{\\textcolor{green!70!black}{+0.59}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & 50.96 & {49.58}\\ \\textsubscript{\\textcolor{red!70!black}{-1.37}} & {51.53}\\ \\textsubscript{\\textcolor{green!70!black}{+0.58}} \\\\\n",
      " & 67.44 & {65.85}\\ \\textsubscript{\\textcolor{red!70!black}{-1.58}} & {68.73}\\ \\textsubscript{\\textcolor{green!70!black}{+1.29}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Lambada} & 55.89 & {53.13}\\ \\textsubscript{\\textcolor{red!70!black}{-2.76}} & {57.03}\\ \\textsubscript{\\textcolor{green!70!black}{+1.14}} \\\\\n",
      " & 7.97 & {8.99}\\ \\textsubscript{\\textcolor{red!70!black}{+1.03}} & {7.64}\\ \\textsubscript{\\textcolor{green!70!black}{-0.32}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 7.31 & {7.40}\\ \\textsubscript{\\textcolor{green!70!black}{+0.08}} & {7.70}\\ \\textsubscript{\\textcolor{green!70!black}{+0.39}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Piqa} & 76.33 & {75.08}\\ \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.17}\\ \\textsubscript{\\textcolor{red!70!black}{-0.16}} \\\\\n",
      " & 76.99 & {75.73}\\ \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.39}\\ \\textsubscript{\\textcolor{red!70!black}{-0.60}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Sciq} & 92.90 & {93.00}\\ \\textsubscript{\\textcolor{green!70!black}{+0.10}} & {94.50}\\ \\textsubscript{\\textcolor{green!70!black}{+1.60}} \\\\\n",
      " & 88.60 & {89.30}\\ \\textsubscript{\\textcolor{green!70!black}{+0.70}} & {91.60}\\ \\textsubscript{\\textcolor{green!70!black}{+3.00}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 44.37 & {44.11}\\ \\textsubscript{\\textcolor{red!70!black}{-0.26}} & {43.91}\\ \\textsubscript{\\textcolor{red!70!black}{-0.46}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 24.28 & {23.36}\\ \\textsubscript{\\textcolor{red!70!black}{-0.92}} & {30.90}\\ \\textsubscript{\\textcolor{green!70!black}{+6.63}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{3}{*}{Wikitext} & 0.66 & {0.69}\\ \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.66}\\ \\textsubscript{\\textcolor{red!70!black}{+0.00}} \\\\\n",
      " & 1.58 & {1.62}\\ \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {1.58}\\ \\textsubscript{\\textcolor{red!70!black}{+0.00}} \\\\\n",
      " & 11.66 & {13.09}\\ \\textsubscript{\\textcolor{red!70!black}{+1.42}} & {11.67}\\ \\textsubscript{\\textcolor{red!70!black}{+0.00}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def create_plot_specify_size(size: str, add_metric_column: bool = True) -> pd.DataFrame:\n",
    "    for json_file in all_json_files:\n",
    "        if size in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                mtp_json_file = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                vanilla_json_file = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                top_json_file = json_file\n",
    "\n",
    "    latex_output, df = generate_latex_comparison_table(\n",
    "        {\"NTP\" : vanilla_json_file, \n",
    "        \"MTP\" : mtp_json_file,\n",
    "        \"TOP\" : top_json_file}, \n",
    "        baseline_source_name=\"NTP\",\n",
    "        size=size,\n",
    "        add_metric_column=add_metric_column\n",
    "    )\n",
    "    print(latex_output)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_plot_specify_size(\"7B\", False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f55ce",
   "metadata": {},
   "source": [
    "# New Table With Combined Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(\n",
    "        json_sources, \n",
    "        baseline_source_name, \n",
    "        precision=2, \n",
    "        size: str | None = None,\n",
    "        add_metric_column: bool = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table. Each\n",
    "    non-baseline cell contains the score and a small, colored delta indicating\n",
    "    the change from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for calculating the delta.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "        size (str | None): Optional size parameter to include in the caption.\n",
    "        add_metric_column (bool): If True, includes the 'Metric' column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "        pd.DataFrame: The processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    # combined_task = [\"Arc Challenge\", \"Arc Easy\", \"Hellaswag\", \"Piqa\", \"Sciq\"]\n",
    "    # remove_acc_norm = df.drop(df[df[\"task\"].isin(combined_task)].index)\n",
    "    # new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n",
    "    # new_rows = [\n",
    "    #     {\n",
    "    #         \"source\": key,\n",
    "    #         \"task\": \"Combined\",\n",
    "    #         \"metric\": \"Acc Norm\",\n",
    "    #         \"value\": value\n",
    "    #     }\n",
    "    #     for key, value in new_values.items()\n",
    "    # ]\n",
    "    # concat_df = pd.concat([remove_acc_norm, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    # df = concat_df\n",
    "    \n",
    "    # Filter out specific metrics\n",
    "    all_metric = set(df[\"metric\"].unique()) - {\"Word Perplexity\", \"Byte Perplexity\"}\n",
    "    df = df[df[\"metric\"].isin(all_metric)]\n",
    "\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Score and Small Delta ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\caption{{Comparison of evaluation results{' for size ' + size if size else ''}. Deltas relative to baseline.}}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    \n",
    "    # --- DUAL COLUMN SETUP ---\n",
    "    column_format = \"l|\"\n",
    "    if add_metric_column:\n",
    "        column_format += \"l|\"\n",
    "    column_format += \"r\" # Baseline column\n",
    "    if other_sources:\n",
    "        # For each other source, create two columns with a small space between them\n",
    "        column_format += \"r@{\\\\hspace{3pt}}l\" * len(other_sources)\n",
    "\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    # Create header with multicolumn for non-baseline sources\n",
    "    header_parts = [\"Task\"]\n",
    "    if add_metric_column:\n",
    "        header_parts.append(\"Metric\")\n",
    "    header_parts.append(baseline_source_name)\n",
    "    for source in other_sources:\n",
    "        header_parts.append(f\"\\\\multicolumn{{2}}{{c}}{{{source}}}\")\n",
    "    latex_parts.append(\" & \".join(header_parts) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val): return val\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                return val * 100 if not is_exception else val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            value_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    delta = compare_val - baseline_val\n",
    "                    rounded_delta = round(delta, precision)\n",
    "\n",
    "                    score_cell = f\"{{{compare_val:.{precision}f}}}\"\n",
    "                    delta_cell = \"\" # Default to empty\n",
    "\n",
    "                    if rounded_delta != 0.0:\n",
    "                        is_good = (rounded_delta > 0) if not any(ex.lower() in metric_name.lower() for ex in exception_to_percentage) else (rounded_delta < 0)\n",
    "                        color = \"green!70!black\" if is_good else \"red!70!black\"\n",
    "                        delta_cell = f\"\\\\textsubscript{{\\\\textcolor{{{color}}}{{{rounded_delta:+.{precision}f}}}}}\"\n",
    "                    \n",
    "                    value_cells.append(score_cell)\n",
    "                    value_cells.append(delta_cell)\n",
    "                else:\n",
    "                    value_cells.append('---')\n",
    "                    value_cells.append('') # Empty cell for delta column\n",
    "            \n",
    "            row_content = \" & \".join(value_cells)\n",
    "            \n",
    "            if add_metric_column:\n",
    "                if j == 0:\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "                else:\n",
    "                    line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else:\n",
    "                if j == 0:\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {row_content} \\\\\\\\\"\n",
    "                else:\n",
    "                    line = f\" & {row_content} \\\\\\\\\"\n",
    "            \n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        # Calculate total columns for cmidrule\n",
    "        total_cols = 1 + (1 if add_metric_column else 0) + 1 + (2 * len(other_sources))\n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{total_cols}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d7259d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results for size 1.8B. Deltas relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|rr@{\\hspace{3pt}}lr@{\\hspace{3pt}}l}\n",
      "\\toprule\n",
      "Task & NTP & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{1}{*}{Combined} & 65.13 & {65.97} & \\textsubscript{\\textcolor{green!70!black}{+0.84}} & {66.99} & \\textsubscript{\\textcolor{green!70!black}{+1.86}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{2}{*}{Lambada} & 49.58 & {47.93} & \\textsubscript{\\textcolor{red!70!black}{-1.65}} & {50.34} & \\textsubscript{\\textcolor{green!70!black}{+0.76}} \\\\\n",
      " & 11.38 & {13.69} & \\textsubscript{\\textcolor{red!70!black}{+2.31}} & {11.19} & \\textsubscript{\\textcolor{green!70!black}{-0.19}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 4.54 & {4.46} & \\textsubscript{\\textcolor{red!70!black}{-0.08}} & {5.37} & \\textsubscript{\\textcolor{green!70!black}{+0.83}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 41.56 & {42.12} & \\textsubscript{\\textcolor{green!70!black}{+0.56}} & {42.53} & \\textsubscript{\\textcolor{green!70!black}{+0.97}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 11.85 & {15.98} & \\textsubscript{\\textcolor{green!70!black}{+4.13}} & {18.93} & \\textsubscript{\\textcolor{green!70!black}{+7.07}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Wikitext} & 0.73 & {0.76} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.73} &  \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_96842/593098147.py:70: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n"
     ]
    }
   ],
   "source": [
    "def create_plot_specify_size(size: str, add_metric_column: bool = True) -> pd.DataFrame:\n",
    "    for json_file in all_json_files:\n",
    "        if size in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                mtp_json_file = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                vanilla_json_file = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                top_json_file = json_file\n",
    "\n",
    "    latex_output, df = generate_latex_comparison_table(\n",
    "        {\"NTP\" : vanilla_json_file, \n",
    "        \"MTP\" : mtp_json_file,\n",
    "        \"TOP\" : top_json_file}, \n",
    "        baseline_source_name=\"NTP\",\n",
    "        size=size,\n",
    "        add_metric_column=add_metric_column\n",
    "    )\n",
    "    print(latex_output)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_plot_specify_size(\"1.8B\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9280eb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arc Challenge', 'Arc Easy', 'Hellaswag', 'Lambada', 'Nq Open',\n",
       "       'Piqa', 'Sciq', 'Social Iqa', 'Triviaqa', 'Wikitext'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"task\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "00844f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_45554/896099635.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df[df[\"task\"].isin(combined_task)][df[\"metric\"] == \"Acc Norm\"][\"value\"].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7071827045421929)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_task = [\"Arc Challenge\", \"Arc Easy\", \"Hellaswag\", \"Piqa\", \"Sciq\"]\n",
    "df[df[\"task\"].isin(combined_task)][df[\"metric\"] == \"Acc Norm\"][\"value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f299df3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>task</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.967722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.558898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.443705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.242755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.663074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>8.993055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.531341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.441146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.233560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>13.086362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.617524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.693787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.643186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.570347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.077008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.439099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.309017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.665974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source        task           metric      value\n",
       "6     NTP     Lambada       Perplexity   7.967722\n",
       "7     NTP     Lambada              Acc   0.558898\n",
       "8     NTP     Nq Open      Exact Match   0.073130\n",
       "13    NTP  Social Iqa              Acc   0.443705\n",
       "14    NTP    Triviaqa      Exact Match   0.242755\n",
       "15    NTP    Wikitext  Word Perplexity  11.663074\n",
       "16    NTP    Wikitext  Byte Perplexity   1.583067\n",
       "17    NTP    Wikitext    Bits Per Byte   0.662723\n",
       "24    MTP     Lambada       Perplexity   8.993055\n",
       "25    MTP     Lambada              Acc   0.531341\n",
       "26    MTP     Nq Open      Exact Match   0.073961\n",
       "31    MTP  Social Iqa              Acc   0.441146\n",
       "32    MTP    Triviaqa      Exact Match   0.233560\n",
       "33    MTP    Wikitext  Word Perplexity  13.086362\n",
       "34    MTP    Wikitext  Byte Perplexity   1.617524\n",
       "35    MTP    Wikitext    Bits Per Byte   0.693787\n",
       "42    TOP     Lambada       Perplexity   7.643186\n",
       "43    TOP     Lambada              Acc   0.570347\n",
       "44    TOP     Nq Open      Exact Match   0.077008\n",
       "49    TOP  Social Iqa              Acc   0.439099\n",
       "50    TOP    Triviaqa      Exact Match   0.309017\n",
       "51    TOP    Wikitext  Word Perplexity  11.665974\n",
       "52    TOP    Wikitext  Byte Perplexity   1.583141\n",
       "53    TOP    Wikitext    Bits Per Byte   0.662790"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_acc_norm = df.drop(df[df[\"task\"].isin(combined_task)].index)\n",
    "remove_acc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'MTP', 'task': 'Combined', 'metric': 'Acc Norm', 'value': 0.7006293719297261}, {'source': 'NTP', 'task': 'Combined', 'metric': 'Acc Norm', 'value': 0.7054061409393804}, {'source': 'TOP', 'task': 'Combined', 'metric': 'Acc Norm', 'value': 0.7155126007574719}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_45554/2034272639.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n"
     ]
    }
   ],
   "source": [
    "# Add new row\n",
    "# new_row = {\n",
    "#     \"source\" : \"Combined\",\n",
    "#     \"task\"\n",
    "# }\n",
    "new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n",
    "new_rows = [\n",
    "    {\n",
    "        \"source\": key,\n",
    "        \"task\": \"Combined\",\n",
    "        \"metric\": \"Acc Norm\",\n",
    "        \"value\": value\n",
    "    }\n",
    "    for key, value in new_values.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b550b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = df.drop(df[df[\"task\"].isin(combined_task)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ff124932",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([dropped_df, pd.DataFrame(new_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f6b7da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>task</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.967722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.558898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.443705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.242755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.663074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>8.993055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.531341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.441146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.233560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>13.086362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.617524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.693787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.643186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.570347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.077008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.439099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.309017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.665974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Acc Norm</td>\n",
       "      <td>0.700629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Acc Norm</td>\n",
       "      <td>0.705406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Acc Norm</td>\n",
       "      <td>0.715513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source        task           metric      value\n",
       "0     NTP     Lambada       Perplexity   7.967722\n",
       "1     NTP     Lambada              Acc   0.558898\n",
       "2     NTP     Nq Open      Exact Match   0.073130\n",
       "3     NTP  Social Iqa              Acc   0.443705\n",
       "4     NTP    Triviaqa      Exact Match   0.242755\n",
       "5     NTP    Wikitext  Word Perplexity  11.663074\n",
       "6     NTP    Wikitext  Byte Perplexity   1.583067\n",
       "7     NTP    Wikitext    Bits Per Byte   0.662723\n",
       "8     MTP     Lambada       Perplexity   8.993055\n",
       "9     MTP     Lambada              Acc   0.531341\n",
       "10    MTP     Nq Open      Exact Match   0.073961\n",
       "11    MTP  Social Iqa              Acc   0.441146\n",
       "12    MTP    Triviaqa      Exact Match   0.233560\n",
       "13    MTP    Wikitext  Word Perplexity  13.086362\n",
       "14    MTP    Wikitext  Byte Perplexity   1.617524\n",
       "15    MTP    Wikitext    Bits Per Byte   0.693787\n",
       "16    TOP     Lambada       Perplexity   7.643186\n",
       "17    TOP     Lambada              Acc   0.570347\n",
       "18    TOP     Nq Open      Exact Match   0.077008\n",
       "19    TOP  Social Iqa              Acc   0.439099\n",
       "20    TOP    Triviaqa      Exact Match   0.309017\n",
       "21    TOP    Wikitext  Word Perplexity  11.665974\n",
       "22    TOP    Wikitext  Byte Perplexity   1.583141\n",
       "23    TOP    Wikitext    Bits Per Byte   0.662790\n",
       "24    MTP    Combined         Acc Norm   0.700629\n",
       "25    NTP    Combined         Acc Norm   0.705406\n",
       "26    TOP    Combined         Acc Norm   0.715513"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f14ad",
   "metadata": {},
   "source": [
    "# Combined Combined Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c93541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_combined_latex_table(\n",
    "        json_sources_by_size, \n",
    "        baseline_source_name, \n",
    "        precision=2, \n",
    "        add_metric_column: bool = True,\n",
    "        combined: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results across different sizes and generates a single\n",
    "    combined LaTeX comparison table.\n",
    "\n",
    "    Args:\n",
    "        json_sources_by_size (dict): A dictionary where keys are sizes (e.g., \"7B\")\n",
    "                                     and values are dictionaries of JSON paths for that size.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "        add_metric_column (bool): If True, includes the 'Metric' column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "        pd.DataFrame: The processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if not json_sources_by_size:\n",
    "        return \"Error: No JSON sources provided.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs from all sizes into a list of records ---\n",
    "    all_records = []\n",
    "    for size, json_sources in json_sources_by_size.items():\n",
    "        if baseline_source_name not in json_sources:\n",
    "            return f\"Error: Baseline source '{baseline_source_name}' not found for size '{size}'.\"\n",
    "        for source_name, json_path in json_sources.items():\n",
    "            try:\n",
    "                with open(json_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                results = data.get(\"results\", {})\n",
    "            except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                print(f\"Warning: Could not process file for '{size} {source_name}'. Error: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for task_name, task_data in results.items():\n",
    "                for key, value in task_data.items():\n",
    "                    if \"alias\" in key or \"stderr\" in key:\n",
    "                        continue\n",
    "                    try:\n",
    "                        numeric_value = float(value)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                    all_records.append({\n",
    "                        \"size\": size,\n",
    "                        \"source\": source_name,\n",
    "                        \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                        \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                        \"value\": numeric_value\n",
    "                    })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    \n",
    "    if combined:\n",
    "        # Handle the \"Combined\" task logic\n",
    "        combined_task_names = [\"Arc Challenge\", \"Arc Easy\", \"Hellaswag\", \"Piqa\", \"Sciq\"]\n",
    "        avg_acc_norm = df[\n",
    "            (df[\"task\"].isin(combined_task_names)) & \n",
    "            (df[\"metric\"] == \"Acc Norm\")\n",
    "        ].groupby([\"size\", \"source\"])[\"value\"].mean().reset_index()\n",
    "        \n",
    "        new_rows = []\n",
    "        for _, row in avg_acc_norm.iterrows():\n",
    "            new_rows.append({\n",
    "                \"size\": row[\"size\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"task\": \"Combined\",\n",
    "                \"metric\": \"Acc Norm\",\n",
    "                \"value\": row[\"value\"]\n",
    "            })\n",
    "        \n",
    "        # Keep original rows that are not part of the combined calculation and add the new combined rows\n",
    "        df_filtered = df.drop(df[df[\"task\"].isin(combined_task_names)].index)\n",
    "        df = pd.concat([df_filtered, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "    # Pivot the table to create a multi-level column structure\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns=['size', 'source'], values='value')\n",
    "    \n",
    "    # Define column order\n",
    "    size_order = [\"340M\", \"1.8B\", \"7B\"]\n",
    "    model_order = [baseline_source_name] + sorted([s for s in list(json_sources_by_size.values())[0] if s != baseline_source_name])\n",
    "    \n",
    "    try:\n",
    "        pivot_df = pivot_df.reindex(columns=pd.MultiIndex.from_product([size_order, model_order], names=['size', 'source']))\n",
    "    except KeyError:\n",
    "        return \"Error: Mismatch in model names across different sizes.\"\n",
    "\n",
    "\n",
    "    # --- 3. Build Combined LaTeX String ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table*}[htbp!]\", # Use table* for full page width\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\small\",\n",
    "        \"\\\\setlength{\\\\tabcolsep}{3pt}\", # Reduce column padding\n",
    "        \"\\\\caption{Combined comparison of evaluation results across model sizes.}\",\n",
    "        \"\\\\label{tab:combined_comparison}\"\n",
    "    ]\n",
    "    \n",
    "    num_models = len(model_order)\n",
    "    num_sizes = len(size_order)\n",
    "    cols_per_model = 2 # score + delta\n",
    "    cols_per_size = cols_per_model * num_models\n",
    "\n",
    "    # --- ALIGNMENT FIX ---\n",
    "    # Define column format to treat every model as a pair of columns for consistent spacing.\n",
    "    column_format = \"l|\"\n",
    "    if add_metric_column:\n",
    "        column_format += \"l|\"\n",
    "    # Use r@{\\hspace{2pt}}l for every model pair for a tighter look\n",
    "    column_format += (\"r@{\\\\hspace{2pt}}l\" * num_models + \"|\") * num_sizes\n",
    "    column_format = column_format[:-1] # Remove the last extra '|'\n",
    "\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    # --- Header Row 1 (Sizes) ---\n",
    "    header1_parts = [\"Task\"]\n",
    "    if add_metric_column:\n",
    "        header1_parts.append(\"Metric\")\n",
    "    for size in size_order:\n",
    "        # Each size group now spans a consistent number of columns\n",
    "        header1_parts.append(f\"\\\\multicolumn{{{cols_per_size}}}{{c|}}{{{size}}}\")\n",
    "    latex_parts.append(\" & \".join(header1_parts) + \" \\\\\\\\\")\n",
    "\n",
    "    # --- Header Row 2 (Models) ---\n",
    "    header2_parts = [\"\"] * (1 + (1 if add_metric_column else 0))\n",
    "    for _ in size_order:\n",
    "        for source in model_order:\n",
    "             # Each model name spans two columns (score + delta)\n",
    "             header2_parts.append(f\"\\\\multicolumn{{{cols_per_model}}}{{c}}{{{source}}}\")\n",
    "    latex_parts.append(\" & \".join(header2_parts) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # --- Placeholder Rows ---\n",
    "    placeholder_cols = cols_per_size * num_sizes\n",
    "    placeholder_text = \" & \".join([\"\"] * (placeholder_cols))\n",
    "    latex_parts.append(f\"Param & {placeholder_text} \\\\\\\\\" if add_metric_column else f\"Param {placeholder_text} \\\\\\\\\")\n",
    "    latex_parts.append(f\"Loss & {placeholder_text} \\\\\\\\\" if add_metric_column else f\"Train {placeholder_text} \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # --- Table Body ---\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val): return val\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                return val * 100 if not is_exception else val\n",
    "\n",
    "            all_cells = []\n",
    "            for size in size_order:\n",
    "                for source in model_order:\n",
    "                    val = format_value(row.get((size, source)), metric_name)\n",
    "                    \n",
    "                    if source == baseline_source_name:\n",
    "                        # Baseline just gets the score and an empty delta cell\n",
    "                        all_cells.append(f\"{val:.{precision}f}\" if pd.notna(val) else '---')\n",
    "                        all_cells.append('')\n",
    "                        continue\n",
    "\n",
    "                    baseline_val = format_value(row.get((size, baseline_source_name)), metric_name)\n",
    "                    if pd.notna(baseline_val) and pd.notna(val):\n",
    "                        delta = val - baseline_val\n",
    "                        rounded_delta = round(delta, precision)\n",
    "\n",
    "                        score_cell = f\"{{{val:.{precision}f}}}\"\n",
    "                        delta_cell = \"\"\n",
    "                        if rounded_delta != 0.0:\n",
    "                            is_good = (rounded_delta > 0) if not any(ex.lower() in metric_name.lower() for ex in exception_to_percentage) else (rounded_delta < 0)\n",
    "                            color = \"green!70!black\" if is_good else \"red!70!black\"\n",
    "                            delta_cell = f\"\\\\textsubscript{{\\\\textcolor{{{color}}}{{{rounded_delta:+.{precision}f}}}}}\"\n",
    "                        \n",
    "                        all_cells.append(score_cell)\n",
    "                        all_cells.append(delta_cell)\n",
    "                    else:\n",
    "                        all_cells.extend(['---', ''])\n",
    "            \n",
    "            row_content = \" & \".join(all_cells)\n",
    "            \n",
    "            # Construct the final row string\n",
    "            line_start = \"\"\n",
    "            if add_metric_column:\n",
    "                if j == 0:\n",
    "                    line_start = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name}\"\n",
    "                else:\n",
    "                    line_start = f\" & {metric_name}\"\n",
    "            else:\n",
    "                if j == 0:\n",
    "                    line_start = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}}\"\n",
    "                else:\n",
    "                    line_start = \"\"\n",
    "            \n",
    "            latex_parts.append(f\"{line_start} & {row_content} \\\\\\\\\")\n",
    "        \n",
    "        total_cols = 1 + (1 if add_metric_column else 0) + placeholder_cols\n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{total_cols}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table*}\"])\n",
    "    print(\"\\n\".join(latex_parts))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c6db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'340M': {'NTP': 'top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json', 'MTP': 'top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json', 'TOP': 'top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json'}, '1.8B': {'TOP': 'top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json', 'MTP': 'top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json', 'NTP': 'top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json'}, '7B': {'TOP': 'top_evals/zaydzuhri__top-7B-4096-model/results_2025-08-20T10-46-34.638307.json', 'NTP': 'top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-20T14-15-37.501701.json', 'MTP': 'top_evals/zaydzuhri__mtp-7B-4096-model/results_2025-08-20T10-59-16.724946.json'}}\n"
     ]
    }
   ],
   "source": [
    "json_sources_by_size = {\"340M\" : {}, \"1.8B\" : {}, \"7B\" : {}}\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    for key, value in json_sources_by_size.items():\n",
    "        if key in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                json_sources_by_size[key][\"MTP\"] = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                json_sources_by_size[key][\"NTP\"] = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                json_sources_by_size[key][\"TOP\"] = json_file\n",
    "\n",
    "print(json_sources_by_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d0e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table*}[htbp!]\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{3pt}\n",
      "\\caption{Combined comparison of evaluation results across model sizes.}\n",
      "\\label{tab:combined_comparison}\n",
      "\\begin{tabular}{l|r@{\\hspace{2pt}}lr@{\\hspace{2pt}}lr@{\\hspace{2pt}}l|r@{\\hspace{2pt}}lr@{\\hspace{2pt}}lr@{\\hspace{2pt}}l|r@{\\hspace{2pt}}lr@{\\hspace{2pt}}lr@{\\hspace{2pt}}l}\n",
      "\\toprule\n",
      "Task & \\multicolumn{6}{c|}{340M} & \\multicolumn{6}{c|}{1.8B} & \\multicolumn{6}{c|}{7B} \\\\\n",
      " & \\multicolumn{2}{c}{NTP} & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} & \\multicolumn{2}{c}{NTP} & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} & \\multicolumn{2}{c}{NTP} & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} \\\\\n",
      "\\midrule\n",
      "Param  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "Train  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & 26.54 &  & {28.07} & \\textsubscript{\\textcolor{green!70!black}{+1.54}} & {28.07} & \\textsubscript{\\textcolor{green!70!black}{+1.54}} & 35.58 &  & {38.40} & \\textsubscript{\\textcolor{green!70!black}{+2.82}} & {39.25} & \\textsubscript{\\textcolor{green!70!black}{+3.67}} & 45.05 &  & {43.69} & \\textsubscript{\\textcolor{red!70!black}{-1.37}} & {44.20} & \\textsubscript{\\textcolor{red!70!black}{-0.85}} \\\\\n",
      " & 28.84 &  & {29.86} & \\textsubscript{\\textcolor{green!70!black}{+1.02}} & {29.35} & \\textsubscript{\\textcolor{green!70!black}{+0.51}} & 38.65 &  & {40.61} & \\textsubscript{\\textcolor{green!70!black}{+1.96}} & {42.32} & \\textsubscript{\\textcolor{green!70!black}{+3.67}} & 45.65 &  & {45.56} & \\textsubscript{\\textcolor{red!70!black}{-0.09}} & {46.42} & \\textsubscript{\\textcolor{green!70!black}{+0.77}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & 60.23 &  & {63.80} & \\textsubscript{\\textcolor{green!70!black}{+3.58}} & {63.26} & \\textsubscript{\\textcolor{green!70!black}{+3.03}} & 72.81 &  & {72.69} & \\textsubscript{\\textcolor{red!70!black}{-0.13}} & {73.48} & \\textsubscript{\\textcolor{green!70!black}{+0.67}} & 77.31 &  & {77.69} & \\textsubscript{\\textcolor{green!70!black}{+0.38}} & {78.03} & \\textsubscript{\\textcolor{green!70!black}{+0.72}} \\\\\n",
      " & 56.52 &  & {58.38} & \\textsubscript{\\textcolor{green!70!black}{+1.85}} & {58.29} & \\textsubscript{\\textcolor{green!70!black}{+1.77}} & 67.05 &  & {70.66} & \\textsubscript{\\textcolor{green!70!black}{+3.62}} & {70.12} & \\textsubscript{\\textcolor{green!70!black}{+3.07}} & 74.03 &  & {73.86} & \\textsubscript{\\textcolor{red!70!black}{-0.17}} & {74.62} & \\textsubscript{\\textcolor{green!70!black}{+0.59}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & 35.52 &  & {35.38} & \\textsubscript{\\textcolor{red!70!black}{-0.14}} & {35.43} & \\textsubscript{\\textcolor{red!70!black}{-0.09}} & 46.03 &  & {44.61} & \\textsubscript{\\textcolor{red!70!black}{-1.41}} & {45.75} & \\textsubscript{\\textcolor{red!70!black}{-0.28}} & 50.96 &  & {49.58} & \\textsubscript{\\textcolor{red!70!black}{-1.37}} & {51.53} & \\textsubscript{\\textcolor{green!70!black}{+0.58}} \\\\\n",
      " & 42.53 &  & {42.73} & \\textsubscript{\\textcolor{green!70!black}{+0.20}} & {43.57} & \\textsubscript{\\textcolor{green!70!black}{+1.04}} & 60.05 &  & {58.29} & \\textsubscript{\\textcolor{red!70!black}{-1.76}} & {60.45} & \\textsubscript{\\textcolor{green!70!black}{+0.40}} & 67.44 &  & {65.85} & \\textsubscript{\\textcolor{red!70!black}{-1.58}} & {68.73} & \\textsubscript{\\textcolor{green!70!black}{+1.29}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{2}{*}{Lambada} & 36.35 &  & {35.32} & \\textsubscript{\\textcolor{red!70!black}{-1.03}} & {37.07} & \\textsubscript{\\textcolor{green!70!black}{+0.72}} & 49.58 &  & {47.93} & \\textsubscript{\\textcolor{red!70!black}{-1.65}} & {50.34} & \\textsubscript{\\textcolor{green!70!black}{+0.76}} & 55.89 &  & {53.13} & \\textsubscript{\\textcolor{red!70!black}{-2.76}} & {57.03} & \\textsubscript{\\textcolor{green!70!black}{+1.14}} \\\\\n",
      " & 30.34 &  & {35.31} & \\textsubscript{\\textcolor{red!70!black}{+4.96}} & {28.76} & \\textsubscript{\\textcolor{green!70!black}{-1.58}} & 11.38 &  & {13.69} & \\textsubscript{\\textcolor{red!70!black}{+2.31}} & {11.19} & \\textsubscript{\\textcolor{green!70!black}{-0.19}} & 7.97 &  & {8.99} & \\textsubscript{\\textcolor{red!70!black}{+1.03}} & {7.64} & \\textsubscript{\\textcolor{green!70!black}{-0.32}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 1.94 &  & {2.35} & \\textsubscript{\\textcolor{green!70!black}{+0.42}} & {2.22} & \\textsubscript{\\textcolor{green!70!black}{+0.28}} & 4.54 &  & {4.46} & \\textsubscript{\\textcolor{red!70!black}{-0.08}} & {5.37} & \\textsubscript{\\textcolor{green!70!black}{+0.83}} & 7.31 &  & {7.40} & \\textsubscript{\\textcolor{green!70!black}{+0.08}} & {7.70} & \\textsubscript{\\textcolor{green!70!black}{+0.39}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{2}{*}{Piqa} & 66.92 &  & {66.27} & \\textsubscript{\\textcolor{red!70!black}{-0.65}} & {67.46} & \\textsubscript{\\textcolor{green!70!black}{+0.54}} & 73.78 &  & {72.20} & \\textsubscript{\\textcolor{red!70!black}{-1.58}} & {73.23} & \\textsubscript{\\textcolor{red!70!black}{-0.54}} & 76.33 &  & {75.08} & \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.17} & \\textsubscript{\\textcolor{red!70!black}{-0.16}} \\\\\n",
      " & 66.65 &  & {66.49} & \\textsubscript{\\textcolor{red!70!black}{-0.16}} & {67.57} & \\textsubscript{\\textcolor{green!70!black}{+0.92}} & 73.50 &  & {73.07} & \\textsubscript{\\textcolor{red!70!black}{-0.44}} & {74.16} & \\textsubscript{\\textcolor{green!70!black}{+0.65}} & 76.99 &  & {75.73} & \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.39} & \\textsubscript{\\textcolor{red!70!black}{-0.60}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{2}{*}{Sciq} & 83.20 &  & {86.30} & \\textsubscript{\\textcolor{green!70!black}{+3.10}} & {85.30} & \\textsubscript{\\textcolor{green!70!black}{+2.10}} & 90.20 &  & {90.70} & \\textsubscript{\\textcolor{green!70!black}{+0.50}} & {91.00} & \\textsubscript{\\textcolor{green!70!black}{+0.80}} & 92.90 &  & {93.00} & \\textsubscript{\\textcolor{green!70!black}{+0.10}} & {94.50} & \\textsubscript{\\textcolor{green!70!black}{+1.60}} \\\\\n",
      " & 74.90 &  & {77.40} & \\textsubscript{\\textcolor{green!70!black}{+2.50}} & {79.80} & \\textsubscript{\\textcolor{green!70!black}{+4.90}} & 86.40 &  & {87.20} & \\textsubscript{\\textcolor{green!70!black}{+0.80}} & {87.90} & \\textsubscript{\\textcolor{green!70!black}{+1.50}} & 88.60 &  & {89.30} & \\textsubscript{\\textcolor{green!70!black}{+0.70}} & {91.60} & \\textsubscript{\\textcolor{green!70!black}{+3.00}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 39.82 &  & {39.00} & \\textsubscript{\\textcolor{red!70!black}{-0.82}} & {39.00} & \\textsubscript{\\textcolor{red!70!black}{-0.82}} & 41.56 &  & {42.12} & \\textsubscript{\\textcolor{green!70!black}{+0.56}} & {42.53} & \\textsubscript{\\textcolor{green!70!black}{+0.97}} & 44.37 &  & {44.11} & \\textsubscript{\\textcolor{red!70!black}{-0.26}} & {43.91} & \\textsubscript{\\textcolor{red!70!black}{-0.46}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 4.93 &  & {2.55} & \\textsubscript{\\textcolor{red!70!black}{-2.37}} & {4.37} & \\textsubscript{\\textcolor{red!70!black}{-0.55}} & 11.85 &  & {15.98} & \\textsubscript{\\textcolor{green!70!black}{+4.13}} & {18.93} & \\textsubscript{\\textcolor{green!70!black}{+7.07}} & 24.28 &  & {23.36} & \\textsubscript{\\textcolor{red!70!black}{-0.92}} & {30.90} & \\textsubscript{\\textcolor{green!70!black}{+6.63}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{3}{*}{Wikitext} & 0.86 &  & {0.88} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.86} &  & 0.73 &  & {0.76} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.73} &  & 0.66 &  & {0.69} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.66} &  \\\\\n",
      " & 1.81 &  & {1.84} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {1.81} &  & 1.66 &  & {1.70} & \\textsubscript{\\textcolor{red!70!black}{+0.04}} & {1.66} &  & 1.58 &  & {1.62} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {1.58} &  \\\\\n",
      " & 23.85 &  & {26.34} & \\textsubscript{\\textcolor{red!70!black}{+2.50}} & {24.01} & \\textsubscript{\\textcolor{red!70!black}{+0.17}} & 15.09 &  & {17.03} & \\textsubscript{\\textcolor{red!70!black}{+1.94}} & {15.04} & \\textsubscript{\\textcolor{green!70!black}{-0.05}} & 11.66 &  & {13.09} & \\textsubscript{\\textcolor{red!70!black}{+1.42}} & {11.67} &  \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "df = generate_combined_latex_table(\n",
    "    json_sources_by_size, \n",
    "    baseline_source_name=\"NTP\",\n",
    "    precision=2,\n",
    "    add_metric_column=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdca50fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results for size 7B. Deltas relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|rr@{\\hspace{3pt}}lr@{\\hspace{3pt}}l}\n",
      "\\toprule\n",
      "Task & NTP & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{1}{*}{Combined} & 70.54 & {70.06} & \\textsubscript{\\textcolor{red!70!black}{-0.48}} & {71.55} & \\textsubscript{\\textcolor{green!70!black}{+1.01}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{2}{*}{Lambada} & 55.89 & {53.13} & \\textsubscript{\\textcolor{red!70!black}{-2.76}} & {57.03} & \\textsubscript{\\textcolor{green!70!black}{+1.14}} \\\\\n",
      " & 7.97 & {8.99} & \\textsubscript{\\textcolor{red!70!black}{+1.03}} & {7.64} & \\textsubscript{\\textcolor{green!70!black}{-0.32}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 7.31 & {7.40} & \\textsubscript{\\textcolor{green!70!black}{+0.08}} & {7.70} & \\textsubscript{\\textcolor{green!70!black}{+0.39}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 44.37 & {44.11} & \\textsubscript{\\textcolor{red!70!black}{-0.26}} & {43.91} & \\textsubscript{\\textcolor{red!70!black}{-0.46}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 24.28 & {23.36} & \\textsubscript{\\textcolor{red!70!black}{-0.92}} & {30.90} & \\textsubscript{\\textcolor{green!70!black}{+6.63}} \\\\\n",
      "\\cmidrule{1-6}\n",
      "\\multirow[c]{1}{*}{Wikitext} & 0.66 & {0.69} & \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.66} &  \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_96842/593098147.py:70: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n"
     ]
    }
   ],
   "source": [
    "def create_plot_specify_size(size: str, add_metric_column: bool = True) -> pd.DataFrame:\n",
    "    for json_file in all_json_files:\n",
    "        if size in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                mtp_json_file = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                vanilla_json_file = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                top_json_file = json_file\n",
    "\n",
    "    latex_output, df = generate_latex_comparison_table(\n",
    "        {\"NTP\" : vanilla_json_file, \n",
    "        \"MTP\" : mtp_json_file,\n",
    "        \"TOP\" : top_json_file}, \n",
    "        baseline_source_name=\"NTP\",\n",
    "        size=size,\n",
    "        add_metric_column=add_metric_column\n",
    "    )\n",
    "    print(latex_output)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_plot_specify_size(\"7B\", False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1509c",
   "metadata": {},
   "source": [
    "# Plot with Ordering and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7884bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_combined_latex_table(\n",
    "        json_sources_by_size, \n",
    "        baseline_source_name, \n",
    "        task_metric_order=None,\n",
    "        precision=2, \n",
    "        add_metric_column: bool = True,\n",
    "        combined: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a single combined LaTeX comparison table.\n",
    "    Allows for specifying the tasks, metrics, and their order in the final table.\n",
    "\n",
    "    Args:\n",
    "        json_sources_by_size (dict): A dictionary where keys are sizes (e.g., \"7B\")\n",
    "                                     and values are dictionaries of JSON paths for that size.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline.\n",
    "        task_metric_order (list, optional): A list of tuples to specify the tasks, \n",
    "                                            metrics, and their order. If None, all tasks/metrics\n",
    "                                            are included. \n",
    "                                            Example:\n",
    "                                            [\n",
    "                                                (\"Lambada\", [\"Acc\", \"Perplexity\"]),\n",
    "                                                (\"Hellaswag\", [\"Acc Norm\"]),\n",
    "                                                (\"Arc Challenge\", [\"Acc Norm\"]),\n",
    "                                            ]\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "        add_metric_column (bool): If True, includes the 'Metric' column.\n",
    "        combined (bool): If True, calculates a \"Combined\" task average from specific tasks.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "        pd.DataFrame: The processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if not json_sources_by_size:\n",
    "        return \"Error: No JSON sources provided.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs from all sizes into a list of records ---\n",
    "    all_records = []\n",
    "    for size, json_sources in json_sources_by_size.items():\n",
    "        if baseline_source_name not in json_sources:\n",
    "            return f\"Error: Baseline source '{baseline_source_name}' not found for size '{size}'.\"\n",
    "        for source_name, json_path in json_sources.items():\n",
    "            try:\n",
    "                with open(json_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                results = data.get(\"results\", {})\n",
    "            except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                print(f\"Warning: Could not process file for '{size} {source_name}'. Error: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for task_name, task_data in results.items():\n",
    "                for key, value in task_data.items():\n",
    "                    if \"alias\" in key or \"stderr\" in key:\n",
    "                        continue\n",
    "                    try:\n",
    "                        numeric_value = float(value)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                    all_records.append({\n",
    "                        \"size\": size,\n",
    "                        \"source\": source_name,\n",
    "                        \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                        \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                        \"value\": numeric_value\n",
    "                    })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create DataFrame and Apply Custom Filtering/Ordering ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    if task_metric_order:\n",
    "        # Create a mapping of (task, metric) to a sorting index\n",
    "        order_map = {}\n",
    "        i = 0\n",
    "        for task, metrics in task_metric_order:\n",
    "            for metric in metrics:\n",
    "                order_map[(task, metric)] = i\n",
    "                i += 1\n",
    "        \n",
    "        # Filter the DataFrame to only include the specified tasks and metrics\n",
    "        df = df[df.apply(lambda row: (row['task'], row['metric']) in order_map, axis=1)].copy()\n",
    "        \n",
    "        # Add a temporary column for sorting and then sort the DataFrame\n",
    "        df['sort_order'] = df.apply(lambda row: order_map.get((row['task'], row['metric'])), axis=1)\n",
    "        df = df.sort_values('sort_order').drop(columns=['sort_order'])\n",
    "\n",
    "    if combined:\n",
    "        # Handle the \"Combined\" task logic\n",
    "        combined_task_names = [\"Arc Challenge\", \"Arc Easy\", \"Hellaswag\", \"Piqa\", \"Sciq\"]\n",
    "        avg_acc_norm = df[\n",
    "            (df[\"task\"].isin(combined_task_names)) & \n",
    "            (df[\"metric\"] == \"Acc Norm\")\n",
    "        ].groupby([\"size\", \"source\"])[\"value\"].mean().reset_index()\n",
    "        \n",
    "        new_rows = []\n",
    "        for _, row in avg_acc_norm.iterrows():\n",
    "            new_rows.append({\n",
    "                \"size\": row[\"size\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"task\": \"Combined\",\n",
    "                \"metric\": \"Acc Norm\",\n",
    "                \"value\": row[\"value\"]\n",
    "            })\n",
    "        \n",
    "        # Keep original rows that are not part of the combined calculation and add the new combined rows\n",
    "        df_filtered = df.drop(df[df[\"task\"].isin(combined_task_names)].index)\n",
    "        df = pd.concat([df_filtered, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "    # Pivot the table, ensuring the sort order from above is maintained\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns=['size', 'source'], values='value', sort=False)\n",
    "    \n",
    "    # Define column order\n",
    "    size_order = [\"340M\", \"1.8B\", \"7B\"]\n",
    "    model_order = [baseline_source_name] + sorted([s for s in list(json_sources_by_size.values())[0] if s != baseline_source_name])\n",
    "    \n",
    "    try:\n",
    "        pivot_df = pivot_df.reindex(columns=pd.MultiIndex.from_product([size_order, model_order], names=['size', 'source']))\n",
    "    except KeyError:\n",
    "        return \"Error: Mismatch in model names across different sizes.\"\n",
    "\n",
    "\n",
    "    # --- 3. Build Combined LaTeX String ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table*}[htbp!]\", # Use table* for full page width\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\small\",\n",
    "        \"\\\\setlength{\\\\tabcolsep}{3pt}\", # Reduce column padding\n",
    "        \"\\\\caption{Combined comparison of evaluation results across model sizes.}\",\n",
    "        \"\\\\label{tab:combined_comparison}\"\n",
    "    ]\n",
    "    \n",
    "    num_models = len(model_order)\n",
    "    num_sizes = len(size_order)\n",
    "    cols_per_model = 2 # score + delta\n",
    "    cols_per_size = cols_per_model * num_models\n",
    "\n",
    "    # Define column format\n",
    "    column_format = \"l|\"\n",
    "    if add_metric_column:\n",
    "        column_format += \"l|\"\n",
    "    column_format += (\"r@{\\\\hspace{2pt}}l\" * num_models + \"|\") * num_sizes\n",
    "    column_format = column_format[:-1] # Remove the last extra '|'\n",
    "\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    # Header Row 1 (Sizes)\n",
    "    header1_parts = [\"Task\"]\n",
    "    if add_metric_column:\n",
    "        header1_parts.append(\"Metric\")\n",
    "    for size in size_order:\n",
    "        header1_parts.append(f\"\\\\multicolumn{{{cols_per_size}}}{{c|}}{{{size}}}\")\n",
    "    latex_parts.append(\" & \".join(header1_parts) + \" \\\\\\\\\")\n",
    "\n",
    "    # Header Row 2 (Models)\n",
    "    header2_parts = [\"\"] * (1 + (1 if add_metric_column else 0))\n",
    "    for _ in size_order:\n",
    "        for source in model_order:\n",
    "              header2_parts.append(f\"\\\\multicolumn{{{cols_per_model}}}{{c}}{{{source}}}\")\n",
    "    latex_parts.append(\" & \".join(header2_parts) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Placeholder Rows\n",
    "    placeholder_cols = cols_per_size * num_sizes\n",
    "    placeholder_text = \" & \".join([\"\"] * (placeholder_cols))\n",
    "    latex_parts.append(f\"Param & {placeholder_text} \\\\\\\\\" if add_metric_column else f\"Param {placeholder_text} \\\\\\\\\")\n",
    "    latex_parts.append(f\"Loss & {placeholder_text} \\\\\\\\\" if add_metric_column else f\"Train {placeholder_text} \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # --- Table Body ---\n",
    "    # Group by task, maintaining the custom order by setting sort=False\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=False)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val): return val\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                return val * 100 if not is_exception else val\n",
    "\n",
    "            all_cells = []\n",
    "            for size in size_order:\n",
    "                for source in model_order:\n",
    "                    val = format_value(row.get((size, source)), metric_name)\n",
    "                    \n",
    "                    if source == baseline_source_name:\n",
    "                        # Baseline just gets the score and an empty delta cell\n",
    "                        all_cells.append(f\"{val:.{precision}f}\" if pd.notna(val) else '---')\n",
    "                        all_cells.append('')\n",
    "                        continue\n",
    "\n",
    "                    baseline_val = format_value(row.get((size, baseline_source_name)), metric_name)\n",
    "                    if pd.notna(baseline_val) and pd.notna(val):\n",
    "                        delta = val - baseline_val\n",
    "                        rounded_delta = round(delta, precision)\n",
    "\n",
    "                        score_cell = f\"{{{val:.{precision}f}}}\"\n",
    "                        delta_cell = \"\"\n",
    "                        if rounded_delta != 0.0:\n",
    "                            is_good = (rounded_delta > 0) if not any(ex.lower() in metric_name.lower() for ex in exception_to_percentage) else (rounded_delta < 0)\n",
    "                            color = \"green!70!black\" if is_good else \"red!70!black\"\n",
    "                            delta_cell = f\"\\\\textsubscript{{\\\\textcolor{{{color}}}{{{rounded_delta:+.{precision}f}}}}}\"\n",
    "                        \n",
    "                        all_cells.append(score_cell)\n",
    "                        all_cells.append(delta_cell)\n",
    "                    else:\n",
    "                        all_cells.extend(['---', ''])\n",
    "            \n",
    "            row_content = \" & \".join(all_cells)\n",
    "            \n",
    "            # Construct the final row string\n",
    "            line_start = \"\"\n",
    "            if add_metric_column:\n",
    "                if j == 0:\n",
    "                    line_start = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name}\"\n",
    "                else:\n",
    "                    line_start = f\" & {metric_name}\"\n",
    "            else:\n",
    "                if j == 0:\n",
    "                    line_start = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}}\"\n",
    "                else:\n",
    "                    line_start = \"\"\n",
    "            \n",
    "            latex_parts.append(f\"{line_start} & {row_content} \\\\\\\\\")\n",
    "        \n",
    "        total_cols = 1 + (1 if add_metric_column else 0) + placeholder_cols\n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{total_cols}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table*}\"])\n",
    "    print(\"\\n\".join(latex_parts))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2becf557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'340M': {'NTP': 'top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json', 'MTP': 'top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json', 'TOP': 'top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json'}, '1.8B': {'TOP': 'top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json', 'MTP': 'top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json', 'NTP': 'top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json'}, '7B': {'TOP': 'top_evals/zaydzuhri__top-7B-4096-model/results_2025-08-20T10-46-34.638307.json', 'NTP': 'top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-20T14-15-37.501701.json', 'MTP': 'top_evals/zaydzuhri__mtp-7B-4096-model/results_2025-08-20T10-59-16.724946.json'}}\n"
     ]
    }
   ],
   "source": [
    "json_sources_by_size = {\"340M\" : {}, \"1.8B\" : {}, \"7B\" : {}}\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    for key, value in json_sources_by_size.items():\n",
    "        if key in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                json_sources_by_size[key][\"MTP\"] = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                json_sources_by_size[key][\"NTP\"] = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                json_sources_by_size[key][\"TOP\"] = json_file\n",
    "\n",
    "print(json_sources_by_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9070c6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Arc Challenge', 'Arc Easy', 'Hellaswag', 'Lambada', 'Nq Open',\n",
       "        'Piqa', 'Sciq', 'Social Iqa', 'Triviaqa', 'Wikitext'], dtype=object),\n",
       " array(['Acc', 'Acc Norm', 'Perplexity', 'Exact Match', 'Word Perplexity',\n",
       "        'Byte Perplexity', 'Bits Per Byte'], dtype=object))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"task\"].unique(), df[\"metric\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2556fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metric_order = [\n",
    "    [\"Lambada\", [\"Acc\", \"Perplexity\"]],\n",
    "    [\"Hellaswag\", [\"Acc Norm\"]],\n",
    "    [\"Arc Challenge\", [\"Acc Norm\"]],\n",
    "    [\"Piqa\", [\"Acc Norm\"]],\n",
    "    [\"Sciq\", [\"Acc Norm\"]],\n",
    "    [\"Social Iqa\", [\"Acc\"]],\n",
    "    [\"Nq Open\", [\"Exact Match\"]],\n",
    "    [\"Triviaqa\", [\"Exact Match\"]],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9191fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table*}[htbp!]\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{3pt}\n",
      "\\caption{Combined comparison of evaluation results across model sizes.}\n",
      "\\label{tab:combined_comparison}\n",
      "\\begin{tabular}{l|r@{\\hspace{2pt}}lr@{\\hspace{2pt}}lr@{\\hspace{2pt}}l|r@{\\hspace{2pt}}lr@{\\hspace{2pt}}lr@{\\hspace{2pt}}l|r@{\\hspace{2pt}}lr@{\\hspace{2pt}}lr@{\\hspace{2pt}}l}\n",
      "\\toprule\n",
      "Task & \\multicolumn{6}{c|}{340M} & \\multicolumn{6}{c|}{1.8B} & \\multicolumn{6}{c|}{7B} \\\\\n",
      " & \\multicolumn{2}{c}{NTP} & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} & \\multicolumn{2}{c}{NTP} & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} & \\multicolumn{2}{c}{NTP} & \\multicolumn{2}{c}{MTP} & \\multicolumn{2}{c}{TOP} \\\\\n",
      "\\midrule\n",
      "Param  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "Train  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Lambada} & 36.35 &  & {35.32} & \\textsubscript{\\textcolor{red!70!black}{-1.03}} & {37.07} & \\textsubscript{\\textcolor{green!70!black}{+0.72}} & 49.58 &  & {47.93} & \\textsubscript{\\textcolor{red!70!black}{-1.65}} & {50.34} & \\textsubscript{\\textcolor{green!70!black}{+0.76}} & 55.89 &  & {53.13} & \\textsubscript{\\textcolor{red!70!black}{-2.76}} & {57.03} & \\textsubscript{\\textcolor{green!70!black}{+1.14}} \\\\\n",
      " & 30.34 &  & {35.31} & \\textsubscript{\\textcolor{red!70!black}{+4.96}} & {28.76} & \\textsubscript{\\textcolor{green!70!black}{-1.58}} & 11.38 &  & {13.69} & \\textsubscript{\\textcolor{red!70!black}{+2.31}} & {11.19} & \\textsubscript{\\textcolor{green!70!black}{-0.19}} & 7.97 &  & {8.99} & \\textsubscript{\\textcolor{red!70!black}{+1.03}} & {7.64} & \\textsubscript{\\textcolor{green!70!black}{-0.32}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Hellaswag} & 42.53 &  & {42.73} & \\textsubscript{\\textcolor{green!70!black}{+0.20}} & {43.57} & \\textsubscript{\\textcolor{green!70!black}{+1.04}} & 60.05 &  & {58.29} & \\textsubscript{\\textcolor{red!70!black}{-1.76}} & {60.45} & \\textsubscript{\\textcolor{green!70!black}{+0.40}} & 67.44 &  & {65.85} & \\textsubscript{\\textcolor{red!70!black}{-1.58}} & {68.73} & \\textsubscript{\\textcolor{green!70!black}{+1.29}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Arc Challenge} & 28.84 &  & {29.86} & \\textsubscript{\\textcolor{green!70!black}{+1.02}} & {29.35} & \\textsubscript{\\textcolor{green!70!black}{+0.51}} & 38.65 &  & {40.61} & \\textsubscript{\\textcolor{green!70!black}{+1.96}} & {42.32} & \\textsubscript{\\textcolor{green!70!black}{+3.67}} & 45.65 &  & {45.56} & \\textsubscript{\\textcolor{red!70!black}{-0.09}} & {46.42} & \\textsubscript{\\textcolor{green!70!black}{+0.77}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Piqa} & 66.65 &  & {66.49} & \\textsubscript{\\textcolor{red!70!black}{-0.16}} & {67.57} & \\textsubscript{\\textcolor{green!70!black}{+0.92}} & 73.50 &  & {73.07} & \\textsubscript{\\textcolor{red!70!black}{-0.44}} & {74.16} & \\textsubscript{\\textcolor{green!70!black}{+0.65}} & 76.99 &  & {75.73} & \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.39} & \\textsubscript{\\textcolor{red!70!black}{-0.60}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Sciq} & 74.90 &  & {77.40} & \\textsubscript{\\textcolor{green!70!black}{+2.50}} & {79.80} & \\textsubscript{\\textcolor{green!70!black}{+4.90}} & 86.40 &  & {87.20} & \\textsubscript{\\textcolor{green!70!black}{+0.80}} & {87.90} & \\textsubscript{\\textcolor{green!70!black}{+1.50}} & 88.60 &  & {89.30} & \\textsubscript{\\textcolor{green!70!black}{+0.70}} & {91.60} & \\textsubscript{\\textcolor{green!70!black}{+3.00}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 39.82 &  & {39.00} & \\textsubscript{\\textcolor{red!70!black}{-0.82}} & {39.00} & \\textsubscript{\\textcolor{red!70!black}{-0.82}} & 41.56 &  & {42.12} & \\textsubscript{\\textcolor{green!70!black}{+0.56}} & {42.53} & \\textsubscript{\\textcolor{green!70!black}{+0.97}} & 44.37 &  & {44.11} & \\textsubscript{\\textcolor{red!70!black}{-0.26}} & {43.91} & \\textsubscript{\\textcolor{red!70!black}{-0.46}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 1.94 &  & {2.35} & \\textsubscript{\\textcolor{green!70!black}{+0.42}} & {2.22} & \\textsubscript{\\textcolor{green!70!black}{+0.28}} & 4.54 &  & {4.46} & \\textsubscript{\\textcolor{red!70!black}{-0.08}} & {5.37} & \\textsubscript{\\textcolor{green!70!black}{+0.83}} & 7.31 &  & {7.40} & \\textsubscript{\\textcolor{green!70!black}{+0.08}} & {7.70} & \\textsubscript{\\textcolor{green!70!black}{+0.39}} \\\\\n",
      "\\cmidrule{1-19}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 4.93 &  & {2.55} & \\textsubscript{\\textcolor{red!70!black}{-2.37}} & {4.37} & \\textsubscript{\\textcolor{red!70!black}{-0.55}} & 11.85 &  & {15.98} & \\textsubscript{\\textcolor{green!70!black}{+4.13}} & {18.93} & \\textsubscript{\\textcolor{green!70!black}{+7.07}} & 24.28 &  & {23.36} & \\textsubscript{\\textcolor{red!70!black}{-0.92}} & {30.90} & \\textsubscript{\\textcolor{green!70!black}{+6.63}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "df = generate_combined_latex_table(\n",
    "    json_sources_by_size, \n",
    "    baseline_source_name=\"NTP\",\n",
    "    precision=2,\n",
    "    task_metric_order=task_metric_order,\n",
    "    add_metric_column=False\n",
    ")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-plotting (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
