{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a9e6c",
   "metadata": {},
   "source": [
    "# Read Data From LM Eval Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b250ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_challenge': ['acc,none', 'acc_norm,none'], 'arc_easy': ['acc,none', 'acc_norm,none'], 'hellaswag': ['acc,none', 'acc_norm,none'], 'lambada_openai': ['perplexity,none', 'acc,none'], 'nq_open': ['exact_match,remove_whitespace'], 'piqa': ['acc,none', 'acc_norm,none'], 'sciq': ['acc,none', 'acc_norm,none'], 'triviaqa': ['exact_match,remove_whitespace'], 'wikitext': ['word_perplexity,none', 'byte_perplexity,none', 'bits_per_byte,none']}\n"
     ]
    }
   ],
   "source": [
    "results = data['results']\n",
    "benchmark_score_name = {}\n",
    "for benchmark, values in results.items():\n",
    "    names = []\n",
    "    for metric, score in values.items():\n",
    "        if metric != \"alias\" and \"stderr\" not in metric:\n",
    "            names.append(metric)\n",
    "    benchmark_score_name[benchmark] = names\n",
    "\n",
    "print(benchmark_score_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e80f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arc_challenge</th>\n",
       "      <th>arc_easy</th>\n",
       "      <th>hellaswag</th>\n",
       "      <th>lambada_openai</th>\n",
       "      <th>nq_open</th>\n",
       "      <th>piqa</th>\n",
       "      <th>sciq</th>\n",
       "      <th>triviaqa</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alias</th>\n",
       "      <td>arc_challenge</td>\n",
       "      <td>arc_easy</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>lambada_openai</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>piqa</td>\n",
       "      <td>sciq</td>\n",
       "      <td>triviaqa</td>\n",
       "      <td>wikitext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc,none</th>\n",
       "      <td>0.450512</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.50946</td>\n",
       "      <td>0.558898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76333</td>\n",
       "      <td>0.929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_stderr,none</th>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_norm,none</th>\n",
       "      <td>0.454778</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.674268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770403</td>\n",
       "      <td>0.886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_norm_stderr,none</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.008992</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      arc_challenge  arc_easy  hellaswag  lambada_openai  \\\n",
       "alias                 arc_challenge  arc_easy  hellaswag  lambada_openai   \n",
       "acc,none                   0.450512  0.773148    0.50946        0.558898   \n",
       "acc_stderr,none             0.01454  0.008594   0.004989        0.006917   \n",
       "acc_norm,none              0.454778  0.740741   0.674268             NaN   \n",
       "acc_norm_stderr,none       0.014552  0.008992   0.004677             NaN   \n",
       "\n",
       "                      nq_open      piqa      sciq  triviaqa  wikitext  \n",
       "alias                 nq_open      piqa      sciq  triviaqa  wikitext  \n",
       "acc,none                  NaN   0.76333     0.929       NaN       NaN  \n",
       "acc_stderr,none           NaN  0.009917  0.008126       NaN       NaN  \n",
       "acc_norm,none             NaN  0.770403     0.886       NaN       NaN  \n",
       "acc_norm_stderr,none      NaN  0.009813  0.010055       NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-19T13-51-01.158842.json\"\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "if \"results\" in data:\n",
    "    df = pd.DataFrame(data[\"results\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65811914",
   "metadata": {},
   "source": [
    "# Create Plot Using Background Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(json_sources, baseline_source_name, precision=2):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table where\n",
    "    each non-baseline cell is colored with a heatmap style based on its\n",
    "    difference from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for coloring.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Direct Cell Coloring ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Comparison of evaluation results. Colors relative to baseline.}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    column_format = \"l|l|\" + \"r\" * len(source_names)\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\", \"Metric\"] + source_names\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            # --- CORRECTED LOGIC FOR PERCENTAGE CONVERSION ---\n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val):\n",
    "                    return val\n",
    "                # Check if the metric is one of the exceptions.\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                # If it's NOT an exception, multiply by 100.\n",
    "                if not is_exception:\n",
    "                    return val * 100\n",
    "                return val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            \n",
    "            # Start the row with the uncolored baseline value\n",
    "            formatted_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            # Process each of the other sources for coloring\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    # For delta calculation, use original non-percentage values for better scaling\n",
    "                    original_baseline = row[baseline_source_name]\n",
    "                    original_compare = row.get(source)\n",
    "                    delta = (original_compare - original_baseline)\n",
    "                    \n",
    "                    # --- Heatmap Logic ---\n",
    "                    max_delta_for_color = 0.1 \n",
    "                    normalized_delta = min(abs(delta) / max_delta_for_color, 1.0)\n",
    "                    intensity = int(normalized_delta * 60)\n",
    "\n",
    "                    is_good = (delta < 0) if \"Perplexity\" in metric_name else (delta > 0)\n",
    "                    color = \"green\" if is_good else \"red\"\n",
    "                    \n",
    "                    cell_str = (f\"\\\\cellcolor{{{color}!{intensity}}}\"\n",
    "                                f\"{{{compare_val:.{precision}f}}}\")\n",
    "                    formatted_cells.append(cell_str)\n",
    "                else:\n",
    "                    formatted_cells.append('---')\n",
    "            \n",
    "            # Join all cells for the final row string\n",
    "            row_content = \" & \".join(formatted_cells)\n",
    "            if j == 0:\n",
    "                line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else:\n",
    "                line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76a7474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 26.54 & \\cellcolor{green!9}{28.07} & \\cellcolor{green!9}{28.07} \\\\\n",
      " & Acc Norm & 28.84 & \\cellcolor{green!6}{29.86} & \\cellcolor{green!3}{29.35} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 60.23 & \\cellcolor{green!21}{63.80} & \\cellcolor{green!18}{63.26} \\\\\n",
      " & Acc Norm & 56.52 & \\cellcolor{green!11}{58.38} & \\cellcolor{green!10}{58.29} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 35.52 & \\cellcolor{red!0}{35.38} & \\cellcolor{red!0}{35.43} \\\\\n",
      " & Acc Norm & 42.53 & \\cellcolor{green!1}{42.73} & \\cellcolor{green!6}{43.57} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 36.35 & \\cellcolor{red!6}{35.32} & \\cellcolor{green!4}{37.07} \\\\\n",
      " & Perplexity & 30.34 & \\cellcolor{red!60}{35.31} & \\cellcolor{green!60}{28.76} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 1.94 & \\cellcolor{green!2}{2.35} & \\cellcolor{green!1}{2.22} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 66.92 & \\cellcolor{red!3}{66.27} & \\cellcolor{green!3}{67.46} \\\\\n",
      " & Acc Norm & 66.65 & \\cellcolor{red!0}{66.49} & \\cellcolor{green!5}{67.57} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 83.20 & \\cellcolor{green!18}{86.30} & \\cellcolor{green!12}{85.30} \\\\\n",
      " & Acc Norm & 74.90 & \\cellcolor{green!15}{77.40} & \\cellcolor{green!29}{79.80} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & 39.82 & \\cellcolor{red!4}{39.00} & \\cellcolor{red!4}{39.00} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 4.93 & \\cellcolor{red!14}{2.55} & \\cellcolor{red!3}{4.37} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.86 & \\cellcolor{green!16}{0.88} & \\cellcolor{green!1}{0.86} \\\\\n",
      " & Byte Perplexity & 1.81 & \\cellcolor{red!20}{1.84} & \\cellcolor{red!1}{1.81} \\\\\n",
      " & Word Perplexity & 23.85 & \\cellcolor{red!60}{26.34} & \\cellcolor{red!60}{24.01} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "mtp_json_file = \"top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json\"\n",
    "vanilla_json_file = \"top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json\"\n",
    "top_json_file = \"top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json\"\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"TOP\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afde031a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "MTP    0.549709\n",
       "NTP    0.538886\n",
       "TOP    0.557166\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['metric'] == 'Acc Norm'].groupby('source')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8f9fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json', 'top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json', 'top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json', 'top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json', 'top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json', 'top_evals/zaydzuhri__top-7B-4096-model/results_2025-08-20T10-46-34.638307.json', 'top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json', 'top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-20T14-15-37.501701.json', 'top_evals/zaydzuhri__mtp-7B-4096-model/results_2025-08-20T10-59-16.724946.json']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "eval_logs_folder = \"top_evals\"\n",
    "all_json_files = glob(f\"{eval_logs_folder}/*/*.json\")\n",
    "print(all_json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b4fc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json\n",
      "top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json\n",
      "top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json\n"
     ]
    }
   ],
   "source": [
    "print(mtp_json_file)\n",
    "print(vanilla_json_file)\n",
    "print(top_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6ebe055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & Top \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 35.58 & \\cellcolor{green!16}{38.40} & \\cellcolor{green!22}{39.25} \\\\\n",
      " & Acc Norm & 38.65 & \\cellcolor{green!11}{40.61} & \\cellcolor{green!22}{42.32} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 72.81 & \\cellcolor{red!0}{72.69} & \\cellcolor{green!4}{73.48} \\\\\n",
      " & Acc Norm & 67.05 & \\cellcolor{green!21}{70.66} & \\cellcolor{green!18}{70.12} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 46.03 & \\cellcolor{red!8}{44.61} & \\cellcolor{red!1}{45.75} \\\\\n",
      " & Acc Norm & 60.05 & \\cellcolor{red!10}{58.29} & \\cellcolor{green!2}{60.45} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 49.58 & \\cellcolor{red!9}{47.93} & \\cellcolor{green!4}{50.34} \\\\\n",
      " & Perplexity & 11.38 & \\cellcolor{red!60}{13.69} & \\cellcolor{green!60}{11.19} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 4.54 & \\cellcolor{red!0}{4.46} & \\cellcolor{green!4}{5.37} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 73.78 & \\cellcolor{red!9}{72.20} & \\cellcolor{red!3}{73.23} \\\\\n",
      " & Acc Norm & 73.50 & \\cellcolor{red!2}{73.07} & \\cellcolor{green!3}{74.16} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 90.20 & \\cellcolor{green!3}{90.70} & \\cellcolor{green!4}{91.00} \\\\\n",
      " & Acc Norm & 86.40 & \\cellcolor{green!4}{87.20} & \\cellcolor{green!9}{87.90} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & 41.56 & \\cellcolor{green!3}{42.12} & \\cellcolor{green!5}{42.53} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 11.85 & \\cellcolor{green!24}{15.98} & \\cellcolor{green!42}{18.93} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.73 & \\cellcolor{green!19}{0.76} & \\cellcolor{red!0}{0.73} \\\\\n",
      " & Byte Perplexity & 1.66 & \\cellcolor{red!22}{1.70} & \\cellcolor{green!0}{1.66} \\\\\n",
      " & Word Perplexity & 15.09 & \\cellcolor{red!60}{17.03} & \\cellcolor{green!31}{15.04} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "size = \"1.8B\"\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    if size in json_file:\n",
    "        if \"mtp\" in json_file:\n",
    "            mtp_json_file = json_file\n",
    "        elif \"vanilla\" in json_file:\n",
    "            vanilla_json_file = json_file\n",
    "        elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "            top_json_file = json_file\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"Top\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b839ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source           task    metric     value\n",
      "1     NTP  Arc Challenge  Acc Norm  0.386519\n",
      "3     NTP       Arc Easy  Acc Norm  0.670455\n",
      "5     NTP      Hellaswag  Acc Norm  0.600478\n",
      "10    NTP           Piqa  Acc Norm  0.735038\n",
      "12    NTP           Sciq  Acc Norm  0.864000\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"metric\"] == \"Acc Norm\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b6c4b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "MTP    0.659666\n",
       "NTP    0.651298\n",
       "Top    0.669883\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['metric'] == 'Acc Norm'].groupby('source')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 45.05 & \\cellcolor{red!8}{43.69} & \\cellcolor{red!5}{44.20} \\\\\n",
      " & Acc Norm & 45.48 & \\cellcolor{green!0}{45.56} & \\cellcolor{green!5}{46.42} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 77.31 & \\cellcolor{green!2}{77.69} & \\cellcolor{green!4}{78.03} \\\\\n",
      " & Acc Norm & 74.07 & \\cellcolor{red!1}{73.86} & \\cellcolor{green!3}{74.62} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 50.95 & \\cellcolor{red!8}{49.58} & \\cellcolor{green!3}{51.53} \\\\\n",
      " & Acc Norm & 67.43 & \\cellcolor{red!9}{65.85} & \\cellcolor{green!7}{68.73} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 55.89 & \\cellcolor{red!16}{53.13} & \\cellcolor{green!6}{57.03} \\\\\n",
      " & Perplexity & 7.97 & \\cellcolor{red!60}{8.99} & \\cellcolor{green!60}{7.64} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 7.31 & \\cellcolor{green!0}{7.40} & \\cellcolor{green!2}{7.70} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 76.33 & \\cellcolor{red!7}{75.08} & \\cellcolor{red!0}{76.17} \\\\\n",
      " & Acc Norm & 77.04 & \\cellcolor{red!7}{75.73} & \\cellcolor{red!3}{76.39} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 92.90 & \\cellcolor{green!0}{93.00} & \\cellcolor{green!9}{94.50} \\\\\n",
      " & Acc Norm & 88.60 & \\cellcolor{green!4}{89.30} & \\cellcolor{green!18}{91.60} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & --- & --- & --- \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 24.28 & \\cellcolor{red!5}{23.36} & \\cellcolor{green!39}{30.90} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.66 & \\cellcolor{green!18}{0.69} & \\cellcolor{green!0}{0.66} \\\\\n",
      " & Byte Perplexity & 1.58 & \\cellcolor{red!20}{1.62} & \\cellcolor{red!0}{1.58} \\\\\n",
      " & Word Perplexity & 11.66 & \\cellcolor{red!60}{13.09} & \\cellcolor{red!1}{11.67} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "size = \"7B\"\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    if size in json_file:\n",
    "        if \"mtp\" in json_file:\n",
    "            mtp_json_file = json_file\n",
    "        elif \"vanilla\" in json_file:\n",
    "            vanilla_json_file = json_file\n",
    "        elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "            top_json_file = json_file\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"TOP\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51988e",
   "metadata": {},
   "source": [
    "# Create Plot Using Small Number at Bottom Corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "82333414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(\n",
    "        json_sources, \n",
    "        baseline_source_name, \n",
    "        precision=2, \n",
    "        size: str | None = None,\n",
    "        add_metric_column: bool = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table. Each\n",
    "    non-baseline cell contains the score and a small, colored delta indicating\n",
    "    the change from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for calculating the delta.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "        size (str | None): Optional size parameter to include in the caption.\n",
    "        add_metric_column (bool): If True, includes the 'Metric' column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "        pd.DataFrame: The processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Score and Small Delta ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\caption{{Comparison of evaluation results{' for size ' + size if size else ''}. Deltas relative to baseline.}}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    \n",
    "    column_format = \"l|\"\n",
    "    if add_metric_column:\n",
    "        column_format += \"l|\"\n",
    "    column_format += \"r\" * len(source_names)\n",
    "    \n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\"]\n",
    "    if add_metric_column:\n",
    "        header_cols.append(\"Metric\")\n",
    "    header_cols += source_names\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val): return val\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                return val * 100 if not is_exception else val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            value_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    delta = compare_val - baseline_val\n",
    "                    is_neutral = abs(delta) < 1e-6\n",
    "\n",
    "                    if is_neutral:\n",
    "                        cell_str = f\"{{{compare_val:.{precision}f}}}\"\n",
    "                    else:\n",
    "                        is_good = (delta > 0) if not any(ex.lower() in metric_name.lower() for ex in exception_to_percentage) else (delta < 0)\n",
    "                        color = \"green!70!black\" if is_good else \"red!70!black\"\n",
    "                        delta_part = f\"\\\\ \\\\textsubscript{{\\\\textcolor{{{color}}}{{{delta:+.{precision}f}}}}}\"\n",
    "                        cell_str = f\"{{{compare_val:.{precision}f}}}{delta_part}\"\n",
    "                    \n",
    "                    value_cells.append(cell_str)\n",
    "                else:\n",
    "                    value_cells.append('---')\n",
    "            \n",
    "            row_content = \" & \".join(value_cells)\n",
    "            \n",
    "            # --- FIXED LOGIC ---\n",
    "            if add_metric_column:\n",
    "                if j == 0: # First row of a task group\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "                else: # Subsequent rows\n",
    "                    line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else: # If not adding metric column\n",
    "                if j == 0:\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {row_content} \\\\\\\\\"\n",
    "                else:\n",
    "                    line = f\" & {row_content} \\\\\\\\\"\n",
    "            \n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "03f438bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results for size 7B. Deltas relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|rrr}\n",
      "\\toprule\n",
      "Task & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & 45.05 & {43.69}\\ \\textsubscript{\\textcolor{red!70!black}{-1.37}} & {44.20}\\ \\textsubscript{\\textcolor{red!70!black}{-0.85}} \\\\\n",
      " & 45.65 & {45.56}\\ \\textsubscript{\\textcolor{red!70!black}{-0.09}} & {46.42}\\ \\textsubscript{\\textcolor{green!70!black}{+0.77}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & 77.31 & {77.69}\\ \\textsubscript{\\textcolor{green!70!black}{+0.38}} & {78.03}\\ \\textsubscript{\\textcolor{green!70!black}{+0.72}} \\\\\n",
      " & 74.03 & {73.86}\\ \\textsubscript{\\textcolor{red!70!black}{-0.17}} & {74.62}\\ \\textsubscript{\\textcolor{green!70!black}{+0.59}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & 50.96 & {49.58}\\ \\textsubscript{\\textcolor{red!70!black}{-1.37}} & {51.53}\\ \\textsubscript{\\textcolor{green!70!black}{+0.58}} \\\\\n",
      " & 67.44 & {65.85}\\ \\textsubscript{\\textcolor{red!70!black}{-1.58}} & {68.73}\\ \\textsubscript{\\textcolor{green!70!black}{+1.29}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Lambada} & 55.89 & {53.13}\\ \\textsubscript{\\textcolor{red!70!black}{-2.76}} & {57.03}\\ \\textsubscript{\\textcolor{green!70!black}{+1.14}} \\\\\n",
      " & 7.97 & {8.99}\\ \\textsubscript{\\textcolor{red!70!black}{+1.03}} & {7.64}\\ \\textsubscript{\\textcolor{green!70!black}{-0.32}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 7.31 & {7.40}\\ \\textsubscript{\\textcolor{green!70!black}{+0.08}} & {7.70}\\ \\textsubscript{\\textcolor{green!70!black}{+0.39}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Piqa} & 76.33 & {75.08}\\ \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.17}\\ \\textsubscript{\\textcolor{red!70!black}{-0.16}} \\\\\n",
      " & 76.99 & {75.73}\\ \\textsubscript{\\textcolor{red!70!black}{-1.25}} & {76.39}\\ \\textsubscript{\\textcolor{red!70!black}{-0.60}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Sciq} & 92.90 & {93.00}\\ \\textsubscript{\\textcolor{green!70!black}{+0.10}} & {94.50}\\ \\textsubscript{\\textcolor{green!70!black}{+1.60}} \\\\\n",
      " & 88.60 & {89.30}\\ \\textsubscript{\\textcolor{green!70!black}{+0.70}} & {91.60}\\ \\textsubscript{\\textcolor{green!70!black}{+3.00}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 44.37 & {44.11}\\ \\textsubscript{\\textcolor{red!70!black}{-0.26}} & {43.91}\\ \\textsubscript{\\textcolor{red!70!black}{-0.46}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 24.28 & {23.36}\\ \\textsubscript{\\textcolor{red!70!black}{-0.92}} & {30.90}\\ \\textsubscript{\\textcolor{green!70!black}{+6.63}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{3}{*}{Wikitext} & 0.66 & {0.69}\\ \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.66}\\ \\textsubscript{\\textcolor{red!70!black}{+0.00}} \\\\\n",
      " & 1.58 & {1.62}\\ \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {1.58}\\ \\textsubscript{\\textcolor{red!70!black}{+0.00}} \\\\\n",
      " & 11.66 & {13.09}\\ \\textsubscript{\\textcolor{red!70!black}{+1.42}} & {11.67}\\ \\textsubscript{\\textcolor{red!70!black}{+0.00}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def create_plot_specify_size(size: str, add_metric_column: bool = True) -> pd.DataFrame:\n",
    "    for json_file in all_json_files:\n",
    "        if size in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                mtp_json_file = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                vanilla_json_file = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                top_json_file = json_file\n",
    "\n",
    "    latex_output, df = generate_latex_comparison_table(\n",
    "        {\"NTP\" : vanilla_json_file, \n",
    "        \"MTP\" : mtp_json_file,\n",
    "        \"TOP\" : top_json_file}, \n",
    "        baseline_source_name=\"NTP\",\n",
    "        size=size,\n",
    "        add_metric_column=add_metric_column\n",
    "    )\n",
    "    print(latex_output)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_plot_specify_size(\"7B\", False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f55ce",
   "metadata": {},
   "source": [
    "# New Table With Combined Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3684fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(\n",
    "        json_sources, \n",
    "        baseline_source_name, \n",
    "        precision=2, \n",
    "        size: str | None = None,\n",
    "        add_metric_column: bool = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table. Each\n",
    "    non-baseline cell contains the score and a small, colored delta indicating\n",
    "    the change from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for calculating the delta.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "        size (str | None): Optional size parameter to include in the caption.\n",
    "        add_metric_column (bool): If True, includes the 'Metric' column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "        pd.DataFrame: The processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    combined_task = [\"Arc Challenge\", \"Arc Easy\", \"Hellaswag\", \"Piqa\", \"Sciq\"]\n",
    "    remove_acc_norm = df.drop(df[df[\"task\"].isin(combined_task)].index)\n",
    "    new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n",
    "    new_rows = [\n",
    "        {\n",
    "            \"source\": key,\n",
    "            \"task\": \"Combined\",\n",
    "            \"metric\": \"Acc Norm\",\n",
    "            \"value\": value\n",
    "        }\n",
    "        for key, value in new_values.items()\n",
    "    ]\n",
    "    concat_df = pd.concat([remove_acc_norm, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    df = concat_df\n",
    "\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Score and Small Delta ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\caption{{Comparison of evaluation results{' for size ' + size if size else ''}. Deltas relative to baseline.}}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    \n",
    "    column_format = \"l|\"\n",
    "    if add_metric_column:\n",
    "        column_format += \"l|\"\n",
    "    column_format += \"r\" * len(source_names)\n",
    "    \n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\"]\n",
    "    if add_metric_column:\n",
    "        header_cols.append(\"Metric\")\n",
    "    header_cols += source_names\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val): return val\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                return val * 100 if not is_exception else val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            value_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    delta = compare_val - baseline_val\n",
    "                    is_neutral = abs(delta) < 1e-6\n",
    "\n",
    "                    if is_neutral:\n",
    "                        cell_str = f\"{{{compare_val:.{precision}f}}}\"\n",
    "                    else:\n",
    "                        is_good = (delta > 0) if not any(ex.lower() in metric_name.lower() for ex in exception_to_percentage) else (delta < 0)\n",
    "                        color = \"green!70!black\" if is_good else \"red!70!black\"\n",
    "                        delta_part = f\"\\\\ \\\\textsubscript{{\\\\textcolor{{{color}}}{{{delta:+.{precision}f}}}}}\"\n",
    "                        cell_str = f\"{{{compare_val:.{precision}f}}}{delta_part}\"\n",
    "                    \n",
    "                    value_cells.append(cell_str)\n",
    "                else:\n",
    "                    value_cells.append('---')\n",
    "            \n",
    "            row_content = \" & \".join(value_cells)\n",
    "            \n",
    "            # --- FIXED LOGIC ---\n",
    "            if add_metric_column:\n",
    "                if j == 0: # First row of a task group\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "                else: # Subsequent rows\n",
    "                    line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else: # If not adding metric column\n",
    "                if j == 0:\n",
    "                    line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {row_content} \\\\\\\\\"\n",
    "                else:\n",
    "                    line = f\" & {row_content} \\\\\\\\\"\n",
    "            \n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4d7259d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results for size 1.8B. Deltas relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|rrr}\n",
      "\\toprule\n",
      "Task & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{1}{*}{Combined} & 65.13 & {65.97}\\ \\textsubscript{\\textcolor{green!70!black}{+0.84}} & {66.99}\\ \\textsubscript{\\textcolor{green!70!black}{+1.86}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{2}{*}{Lambada} & 49.58 & {47.93}\\ \\textsubscript{\\textcolor{red!70!black}{-1.65}} & {50.34}\\ \\textsubscript{\\textcolor{green!70!black}{+0.76}} \\\\\n",
      " & 11.38 & {13.69}\\ \\textsubscript{\\textcolor{red!70!black}{+2.31}} & {11.19}\\ \\textsubscript{\\textcolor{green!70!black}{-0.19}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Nq Open} & 4.54 & {4.46}\\ \\textsubscript{\\textcolor{red!70!black}{-0.08}} & {5.37}\\ \\textsubscript{\\textcolor{green!70!black}{+0.83}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & 41.56 & {42.12}\\ \\textsubscript{\\textcolor{green!70!black}{+0.56}} & {42.53}\\ \\textsubscript{\\textcolor{green!70!black}{+0.97}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & 11.85 & {15.98}\\ \\textsubscript{\\textcolor{green!70!black}{+4.13}} & {18.93}\\ \\textsubscript{\\textcolor{green!70!black}{+7.07}} \\\\\n",
      "\\cmidrule{1-4}\n",
      "\\multirow[c]{3}{*}{Wikitext} & 0.73 & {0.76}\\ \\textsubscript{\\textcolor{red!70!black}{+0.03}} & {0.73}\\ \\textsubscript{\\textcolor{green!70!black}{-0.00}} \\\\\n",
      " & 1.66 & {1.70}\\ \\textsubscript{\\textcolor{red!70!black}{+0.04}} & {1.66}\\ \\textsubscript{\\textcolor{green!70!black}{-0.00}} \\\\\n",
      " & 15.09 & {17.03}\\ \\textsubscript{\\textcolor{red!70!black}{+1.94}} & {15.04}\\ \\textsubscript{\\textcolor{green!70!black}{-0.05}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_45554/3265094588.py:70: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n"
     ]
    }
   ],
   "source": [
    "def create_plot_specify_size(size: str, add_metric_column: bool = True) -> pd.DataFrame:\n",
    "    for json_file in all_json_files:\n",
    "        if size in json_file:\n",
    "            if \"mtp\" in json_file:\n",
    "                mtp_json_file = json_file\n",
    "            elif \"vanilla\" in json_file:\n",
    "                vanilla_json_file = json_file\n",
    "            elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "                top_json_file = json_file\n",
    "\n",
    "    latex_output, df = generate_latex_comparison_table(\n",
    "        {\"NTP\" : vanilla_json_file, \n",
    "        \"MTP\" : mtp_json_file,\n",
    "        \"TOP\" : top_json_file}, \n",
    "        baseline_source_name=\"NTP\",\n",
    "        size=size,\n",
    "        add_metric_column=add_metric_column\n",
    "    )\n",
    "    print(latex_output)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_plot_specify_size(\"1.8B\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9280eb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arc Challenge', 'Arc Easy', 'Hellaswag', 'Lambada', 'Nq Open',\n",
       "       'Piqa', 'Sciq', 'Social Iqa', 'Triviaqa', 'Wikitext'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"task\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "00844f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_45554/896099635.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df[df[\"task\"].isin(combined_task)][df[\"metric\"] == \"Acc Norm\"][\"value\"].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7071827045421929)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_task = [\"Arc Challenge\", \"Arc Easy\", \"Hellaswag\", \"Piqa\", \"Sciq\"]\n",
    "df[df[\"task\"].isin(combined_task)][df[\"metric\"] == \"Acc Norm\"][\"value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f299df3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>task</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.967722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.558898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.443705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.242755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.663074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>8.993055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.531341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.441146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.233560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>13.086362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.617524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.693787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.643186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.570347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.077008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.439099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.309017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.665974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source        task           metric      value\n",
       "6     NTP     Lambada       Perplexity   7.967722\n",
       "7     NTP     Lambada              Acc   0.558898\n",
       "8     NTP     Nq Open      Exact Match   0.073130\n",
       "13    NTP  Social Iqa              Acc   0.443705\n",
       "14    NTP    Triviaqa      Exact Match   0.242755\n",
       "15    NTP    Wikitext  Word Perplexity  11.663074\n",
       "16    NTP    Wikitext  Byte Perplexity   1.583067\n",
       "17    NTP    Wikitext    Bits Per Byte   0.662723\n",
       "24    MTP     Lambada       Perplexity   8.993055\n",
       "25    MTP     Lambada              Acc   0.531341\n",
       "26    MTP     Nq Open      Exact Match   0.073961\n",
       "31    MTP  Social Iqa              Acc   0.441146\n",
       "32    MTP    Triviaqa      Exact Match   0.233560\n",
       "33    MTP    Wikitext  Word Perplexity  13.086362\n",
       "34    MTP    Wikitext  Byte Perplexity   1.617524\n",
       "35    MTP    Wikitext    Bits Per Byte   0.693787\n",
       "42    TOP     Lambada       Perplexity   7.643186\n",
       "43    TOP     Lambada              Acc   0.570347\n",
       "44    TOP     Nq Open      Exact Match   0.077008\n",
       "49    TOP  Social Iqa              Acc   0.439099\n",
       "50    TOP    Triviaqa      Exact Match   0.309017\n",
       "51    TOP    Wikitext  Word Perplexity  11.665974\n",
       "52    TOP    Wikitext  Byte Perplexity   1.583141\n",
       "53    TOP    Wikitext    Bits Per Byte   0.662790"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_acc_norm = df.drop(df[df[\"task\"].isin(combined_task)].index)\n",
    "remove_acc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'MTP', 'task': 'Combined', 'metric': 'Acc Norm', 'value': 0.7006293719297261}, {'source': 'NTP', 'task': 'Combined', 'metric': 'Acc Norm', 'value': 0.7054061409393804}, {'source': 'TOP', 'task': 'Combined', 'metric': 'Acc Norm', 'value': 0.7155126007574719}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b6xvnbkj05l2286tn343qd1r0000gn/T/ipykernel_45554/2034272639.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n"
     ]
    }
   ],
   "source": [
    "# Add new row\n",
    "# new_row = {\n",
    "#     \"source\" : \"Combined\",\n",
    "#     \"task\"\n",
    "# }\n",
    "new_values = df[df[\"task\"].isin(combined_task)][(df[\"metric\"] == \"Acc Norm\")].groupby(\"source\")[\"value\"].mean().to_dict()\n",
    "new_rows = [\n",
    "    {\n",
    "        \"source\": key,\n",
    "        \"task\": \"Combined\",\n",
    "        \"metric\": \"Acc Norm\",\n",
    "        \"value\": value\n",
    "    }\n",
    "    for key, value in new_values.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b550b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = df.drop(df[df[\"task\"].isin(combined_task)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ff124932",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([dropped_df, pd.DataFrame(new_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f6b7da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>task</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.967722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.558898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.443705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.242755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.663074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>8.993055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.531341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.073961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.441146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.233560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>13.086362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.617524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.693787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>7.643186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Lambada</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.570347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Nq Open</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.077008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Social Iqa</td>\n",
       "      <td>Acc</td>\n",
       "      <td>0.439099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Triviaqa</td>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.309017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Word Perplexity</td>\n",
       "      <td>11.665974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Byte Perplexity</td>\n",
       "      <td>1.583141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Wikitext</td>\n",
       "      <td>Bits Per Byte</td>\n",
       "      <td>0.662790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MTP</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Acc Norm</td>\n",
       "      <td>0.700629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NTP</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Acc Norm</td>\n",
       "      <td>0.705406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TOP</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Acc Norm</td>\n",
       "      <td>0.715513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source        task           metric      value\n",
       "0     NTP     Lambada       Perplexity   7.967722\n",
       "1     NTP     Lambada              Acc   0.558898\n",
       "2     NTP     Nq Open      Exact Match   0.073130\n",
       "3     NTP  Social Iqa              Acc   0.443705\n",
       "4     NTP    Triviaqa      Exact Match   0.242755\n",
       "5     NTP    Wikitext  Word Perplexity  11.663074\n",
       "6     NTP    Wikitext  Byte Perplexity   1.583067\n",
       "7     NTP    Wikitext    Bits Per Byte   0.662723\n",
       "8     MTP     Lambada       Perplexity   8.993055\n",
       "9     MTP     Lambada              Acc   0.531341\n",
       "10    MTP     Nq Open      Exact Match   0.073961\n",
       "11    MTP  Social Iqa              Acc   0.441146\n",
       "12    MTP    Triviaqa      Exact Match   0.233560\n",
       "13    MTP    Wikitext  Word Perplexity  13.086362\n",
       "14    MTP    Wikitext  Byte Perplexity   1.617524\n",
       "15    MTP    Wikitext    Bits Per Byte   0.693787\n",
       "16    TOP     Lambada       Perplexity   7.643186\n",
       "17    TOP     Lambada              Acc   0.570347\n",
       "18    TOP     Nq Open      Exact Match   0.077008\n",
       "19    TOP  Social Iqa              Acc   0.439099\n",
       "20    TOP    Triviaqa      Exact Match   0.309017\n",
       "21    TOP    Wikitext  Word Perplexity  11.665974\n",
       "22    TOP    Wikitext  Byte Perplexity   1.583141\n",
       "23    TOP    Wikitext    Bits Per Byte   0.662790\n",
       "24    MTP    Combined         Acc Norm   0.700629\n",
       "25    NTP    Combined         Acc Norm   0.705406\n",
       "26    TOP    Combined         Acc Norm   0.715513"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-plotting (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
