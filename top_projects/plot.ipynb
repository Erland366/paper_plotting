{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a9e6c",
   "metadata": {},
   "source": [
    "# Read Data From LM Eval Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9461a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-19T13-51-01.158842.json\"\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b250ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_challenge': ['acc,none', 'acc_norm,none'], 'arc_easy': ['acc,none', 'acc_norm,none'], 'hellaswag': ['acc,none', 'acc_norm,none'], 'lambada_openai': ['perplexity,none', 'acc,none'], 'nq_open': ['exact_match,remove_whitespace'], 'piqa': ['acc,none', 'acc_norm,none'], 'sciq': ['acc,none', 'acc_norm,none'], 'triviaqa': ['exact_match,remove_whitespace'], 'wikitext': ['word_perplexity,none', 'byte_perplexity,none', 'bits_per_byte,none']}\n"
     ]
    }
   ],
   "source": [
    "results = data['results']\n",
    "benchmark_score_name = {}\n",
    "for benchmark, values in results.items():\n",
    "    names = []\n",
    "    for metric, score in values.items():\n",
    "        if metric != \"alias\" and \"stderr\" not in metric:\n",
    "            names.append(metric)\n",
    "    benchmark_score_name[benchmark] = names\n",
    "\n",
    "print(benchmark_score_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e80f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arc_challenge</th>\n",
       "      <th>arc_easy</th>\n",
       "      <th>hellaswag</th>\n",
       "      <th>lambada_openai</th>\n",
       "      <th>nq_open</th>\n",
       "      <th>piqa</th>\n",
       "      <th>sciq</th>\n",
       "      <th>triviaqa</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alias</th>\n",
       "      <td>arc_challenge</td>\n",
       "      <td>arc_easy</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>lambada_openai</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>piqa</td>\n",
       "      <td>sciq</td>\n",
       "      <td>triviaqa</td>\n",
       "      <td>wikitext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc,none</th>\n",
       "      <td>0.450512</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.50946</td>\n",
       "      <td>0.558898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76333</td>\n",
       "      <td>0.929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_stderr,none</th>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_norm,none</th>\n",
       "      <td>0.454778</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.674268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770403</td>\n",
       "      <td>0.886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_norm_stderr,none</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.008992</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      arc_challenge  arc_easy  hellaswag  lambada_openai  \\\n",
       "alias                 arc_challenge  arc_easy  hellaswag  lambada_openai   \n",
       "acc,none                   0.450512  0.773148    0.50946        0.558898   \n",
       "acc_stderr,none             0.01454  0.008594   0.004989        0.006917   \n",
       "acc_norm,none              0.454778  0.740741   0.674268             NaN   \n",
       "acc_norm_stderr,none       0.014552  0.008992   0.004677             NaN   \n",
       "\n",
       "                      nq_open      piqa      sciq  triviaqa  wikitext  \n",
       "alias                 nq_open      piqa      sciq  triviaqa  wikitext  \n",
       "acc,none                  NaN   0.76333     0.929       NaN       NaN  \n",
       "acc_stderr,none           NaN  0.009917  0.008126       NaN       NaN  \n",
       "acc_norm,none             NaN  0.770403     0.886       NaN       NaN  \n",
       "acc_norm_stderr,none      NaN  0.009813  0.010055       NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-19T13-51-01.158842.json\"\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "if \"results\" in data:\n",
    "    df = pd.DataFrame(data[\"results\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. $\\Delta$ = Model - Baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & Vanilla & Non-vanilla & $\\Delta$ \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 0.45 & 0.46 & \\textcolor{green!70!black}{+0.01} \\\\\n",
      " & Acc Norm & 0.45 & 0.46 & \\textcolor{green!70!black}{+0.01} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 0.77 & 0.76 & \\textcolor{red!70!black}{-0.01} \\\\\n",
      " & Acc Norm & 0.74 & 0.75 & \\textcolor{green!70!black}{+0.01} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 0.51 & 0.52 & \\textcolor{green!70!black}{+0.01} \\\\\n",
      " & Acc Norm & 0.67 & 0.66 & \\textcolor{red!70!black}{-0.01} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 0.56 & 0.46 & \\textcolor{red!70!black}{-0.10} \\\\\n",
      " & Perplexity & 7.97 & 7.87 & \\textcolor{green!70!black}{-0.10} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 0.07 & 0.08 & \\textcolor{green!70!black}{+0.01} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 0.76 & 0.76 & \\textcolor{black}{+0.00} \\\\\n",
      " & Acc Norm & 0.77 & 0.77 & \\textcolor{black}{+0.00} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 0.93 & 0.93 & \\textcolor{black}{+0.00} \\\\\n",
      " & Acc Norm & 0.89 & 0.89 & \\textcolor{black}{+0.00} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 0.24 & 0.24 & \\textcolor{black}{+0.00} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.66 & 0.66 & \\textcolor{black}{+0.00} \\\\\n",
      " & Byte Perplexity & 1.58 & 1.58 & \\textcolor{black}{+0.00} \\\\\n",
      " & Word Perplexity & 11.66 & 11.66 & \\textcolor{black}{+0.00} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def generate_latex_comparison_table(json_sources, baseline_source_name, precision=2):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results, uses pandas to structure the data, and\n",
    "    generates a LaTeX comparison table with a heatmap-style delta column.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for calculating the delta.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = [s for s in json_sources if s != baseline_source_name]\n",
    "    valid_columns = [baseline_source_name] + sorted([s for s in other_sources if s in pivot_df.columns])\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Calculate Delta and Build LaTeX String ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Comparison of evaluation results. $\\\\Delta$ = Model - Baseline.}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    column_format = \"l|l|\" + \"r\" * (len(source_names) + (len(source_names) > 1))\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\", \"Metric\"] + source_names\n",
    "    if len(source_names) > 1:\n",
    "        header_cols.append(\"$\\\\Delta$\")\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            row_values = [f\"{val:.{precision}f}\" if pd.notna(val) else '---' for val in row]\n",
    "            \n",
    "            delta_str = \"\"\n",
    "            if len(source_names) > 1:\n",
    "                baseline_val = row[baseline_source_name]\n",
    "                compare_val = row[source_names[1]]\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    delta = compare_val - baseline_val\n",
    "                    \n",
    "                    # --- Heatmap Logic ---\n",
    "                    # Normalize delta to a 0-1 range for intensity, cap at a reasonable max\n",
    "                    # A value of 0.1 delta (10 percentage points) is considered max intensity\n",
    "                    max_delta_for_color = 0.1 \n",
    "                    normalized_delta = min(abs(delta) / max_delta_for_color, 1.0)\n",
    "                    # Intensity from 0 (white) to 60 (strong color)\n",
    "                    intensity = int(normalized_delta * 60) \n",
    "\n",
    "                    is_good = (delta < 0) if \"Perplexity\" in metric_name else (delta > 0)\n",
    "                    color = \"green\" if is_good else \"red\"\n",
    "                    \n",
    "                    # Use \\cellcolor for the background heatmap\n",
    "                    delta_str = (f\" & \\\\cellcolor{{{color}!{intensity}}}\"\n",
    "                                 f\"{{{delta:+.{precision}f}}}\")\n",
    "                else:\n",
    "                    delta_str = \" & ---\"\n",
    "\n",
    "\n",
    "            if j == 0:\n",
    "                line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {' & '.join(row_values)}{delta_str} \\\\\\\\\"\n",
    "            else:\n",
    "                line = f\" & {metric_name} & {' & '.join(row_values)}{delta_str} \\\\\\\\\"\n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    print(\"Hey I modify something here\")\n",
    "    return \"\\n\".join(latex_parts)\n",
    "\n",
    "non_vanilla_json_file = \"top_evals/zaydzuhri__vanilla-7B-4096-model-test/results_2025-08-19T13-51-01.158842.json\"\n",
    "latex_output = generate_latex_comparison_table({\"Vanilla\" : json_file, \"Non-vanilla\" : non_vanilla_json_file}, baseline_source_name=\"Vanilla\")\n",
    "print(latex_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-plotting (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
