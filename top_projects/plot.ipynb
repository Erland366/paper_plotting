{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a9e6c",
   "metadata": {},
   "source": [
    "# Read Data From LM Eval Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9461a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-19T13-51-01.158842.json\"\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b250ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_challenge': ['acc,none', 'acc_norm,none'], 'arc_easy': ['acc,none', 'acc_norm,none'], 'hellaswag': ['acc,none', 'acc_norm,none'], 'lambada_openai': ['perplexity,none', 'acc,none'], 'nq_open': ['exact_match,remove_whitespace'], 'piqa': ['acc,none', 'acc_norm,none'], 'sciq': ['acc,none', 'acc_norm,none'], 'triviaqa': ['exact_match,remove_whitespace'], 'wikitext': ['word_perplexity,none', 'byte_perplexity,none', 'bits_per_byte,none']}\n"
     ]
    }
   ],
   "source": [
    "results = data['results']\n",
    "benchmark_score_name = {}\n",
    "for benchmark, values in results.items():\n",
    "    names = []\n",
    "    for metric, score in values.items():\n",
    "        if metric != \"alias\" and \"stderr\" not in metric:\n",
    "            names.append(metric)\n",
    "    benchmark_score_name[benchmark] = names\n",
    "\n",
    "print(benchmark_score_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e80f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arc_challenge</th>\n",
       "      <th>arc_easy</th>\n",
       "      <th>hellaswag</th>\n",
       "      <th>lambada_openai</th>\n",
       "      <th>nq_open</th>\n",
       "      <th>piqa</th>\n",
       "      <th>sciq</th>\n",
       "      <th>triviaqa</th>\n",
       "      <th>wikitext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alias</th>\n",
       "      <td>arc_challenge</td>\n",
       "      <td>arc_easy</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>lambada_openai</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>piqa</td>\n",
       "      <td>sciq</td>\n",
       "      <td>triviaqa</td>\n",
       "      <td>wikitext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc,none</th>\n",
       "      <td>0.450512</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.50946</td>\n",
       "      <td>0.558898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76333</td>\n",
       "      <td>0.929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_stderr,none</th>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_norm,none</th>\n",
       "      <td>0.454778</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.674268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770403</td>\n",
       "      <td>0.886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc_norm_stderr,none</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.008992</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      arc_challenge  arc_easy  hellaswag  lambada_openai  \\\n",
       "alias                 arc_challenge  arc_easy  hellaswag  lambada_openai   \n",
       "acc,none                   0.450512  0.773148    0.50946        0.558898   \n",
       "acc_stderr,none             0.01454  0.008594   0.004989        0.006917   \n",
       "acc_norm,none              0.454778  0.740741   0.674268             NaN   \n",
       "acc_norm_stderr,none       0.014552  0.008992   0.004677             NaN   \n",
       "\n",
       "                      nq_open      piqa      sciq  triviaqa  wikitext  \n",
       "alias                 nq_open      piqa      sciq  triviaqa  wikitext  \n",
       "acc,none                  NaN   0.76333     0.929       NaN       NaN  \n",
       "acc_stderr,none           NaN  0.009917  0.008126       NaN       NaN  \n",
       "acc_norm,none             NaN  0.770403     0.886       NaN       NaN  \n",
       "acc_norm_stderr,none      NaN  0.009813  0.010055       NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = \"top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-19T13-51-01.158842.json\"\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "if \"results\" in data:\n",
    "    df = pd.DataFrame(data[\"results\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7000c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exception_to_percentage = [\n",
    "    \"Perplexity\",\n",
    "    \"Bits\"\n",
    "]\n",
    "\n",
    "def generate_latex_comparison_table(json_sources, baseline_source_name, precision=2):\n",
    "    \"\"\"\n",
    "    Parses multiple JSON results and generates a LaTeX comparison table where\n",
    "    each non-baseline cell is colored with a heatmap style based on its\n",
    "    difference from the baseline.\n",
    "\n",
    "    Args:\n",
    "        json_sources (dict): A dictionary where keys are source names (e.g., \"Model A\")\n",
    "                             and values are the JSON path.\n",
    "        baseline_source_name (str): The name of the source to use as the baseline\n",
    "                                    for coloring.\n",
    "        precision (int): The number of decimal places for the scores.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the generated LaTeX table.\n",
    "    \"\"\"\n",
    "    if baseline_source_name not in json_sources:\n",
    "        return \"Error: Baseline source name not found in json_sources.\"\n",
    "\n",
    "    # --- 1. Parse all JSONs into a list of records ---\n",
    "    all_records = []\n",
    "    for source_name, json_path in json_sources.items():\n",
    "        try:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            results = data.get(\"results\", {})\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Warning: Could not read or parse file for source '{source_name}' at {json_path}. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for task_name, task_data in results.items():\n",
    "            for key, value in task_data.items():\n",
    "                if \"alias\" in key or \"stderr\" in key:\n",
    "                    continue\n",
    "                try:\n",
    "                    numeric_value = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                all_records.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"task\": task_name.replace('_', ' ').replace('openai', '').strip().title(),\n",
    "                    \"metric\": key.split(',')[0].replace('_', ' ').title(),\n",
    "                    \"value\": numeric_value\n",
    "                })\n",
    "\n",
    "    if not all_records:\n",
    "        return \"No valid data found to generate a table.\"\n",
    "\n",
    "    # --- 2. Create and Pivot DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    pivot_df = df.pivot_table(index=['task', 'metric'], columns='source', values='value')\n",
    "    other_sources = sorted([s for s in json_sources if s != baseline_source_name])\n",
    "    valid_columns = [baseline_source_name] + [s for s in other_sources if s in pivot_df.columns]\n",
    "    pivot_df = pivot_df[valid_columns]\n",
    "\n",
    "    # --- 3. Build LaTeX String with Direct Cell Coloring ---\n",
    "    latex_parts = [\n",
    "        \"% Add this to your LaTeX preamble: \\\\usepackage[table]{xcolor} \\\\usepackage{multirow} \\\\usepackage{booktabs}\",\n",
    "        \"\\\\begin{table}[htbp!]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Comparison of evaluation results. Colors relative to baseline.}\",\n",
    "        \"\\\\label{tab:generated_comparison}\"\n",
    "    ]\n",
    "    source_names = pivot_df.columns.tolist()\n",
    "    column_format = \"l|l|\" + \"r\" * len(source_names)\n",
    "    latex_parts.append(f\"\\\\begin{{tabular}}{{{column_format}}}\")\n",
    "    latex_parts.append(\"\\\\toprule\")\n",
    "    \n",
    "    header_cols = [\"Task\", \"Metric\"] + source_names\n",
    "    latex_parts.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "    latex_parts.append(\"\\\\midrule\")\n",
    "\n",
    "    # Table Body\n",
    "    for i, (task_name, group) in enumerate(pivot_df.groupby(level='task', sort=True)):\n",
    "        num_metrics = len(group)\n",
    "        for j, ((_, metric_name), row) in enumerate(group.sort_index().iterrows()):\n",
    "            \n",
    "            # --- CORRECTED LOGIC FOR PERCENTAGE CONVERSION ---\n",
    "            def format_value(val, metric):\n",
    "                if pd.isna(val):\n",
    "                    return val\n",
    "                # Check if the metric is one of the exceptions.\n",
    "                is_exception = any(ex.lower() in metric.lower() for ex in exception_to_percentage)\n",
    "                # If it's NOT an exception, multiply by 100.\n",
    "                if not is_exception:\n",
    "                    return val * 100\n",
    "                return val\n",
    "\n",
    "            baseline_val = format_value(row[baseline_source_name], metric_name)\n",
    "            \n",
    "            # Start the row with the uncolored baseline value\n",
    "            formatted_cells = [f\"{baseline_val:.{precision}f}\" if pd.notna(baseline_val) else '---']\n",
    "\n",
    "            # Process each of the other sources for coloring\n",
    "            for source in other_sources:\n",
    "                compare_val = format_value(row.get(source), metric_name)\n",
    "                \n",
    "                if pd.notna(baseline_val) and pd.notna(compare_val):\n",
    "                    # For delta calculation, use original non-percentage values for better scaling\n",
    "                    original_baseline = row[baseline_source_name]\n",
    "                    original_compare = row.get(source)\n",
    "                    delta = (original_compare - original_baseline)\n",
    "                    \n",
    "                    # --- Heatmap Logic ---\n",
    "                    max_delta_for_color = 0.1 \n",
    "                    normalized_delta = min(abs(delta) / max_delta_for_color, 1.0)\n",
    "                    intensity = int(normalized_delta * 60)\n",
    "\n",
    "                    is_good = (delta < 0) if \"Perplexity\" in metric_name else (delta > 0)\n",
    "                    color = \"green\" if is_good else \"red\"\n",
    "                    \n",
    "                    cell_str = (f\"\\\\cellcolor{{{color}!{intensity}}}\"\n",
    "                                f\"{{{compare_val:.{precision}f}}}\")\n",
    "                    formatted_cells.append(cell_str)\n",
    "                else:\n",
    "                    formatted_cells.append('---')\n",
    "            \n",
    "            # Join all cells for the final row string\n",
    "            row_content = \" & \".join(formatted_cells)\n",
    "            if j == 0:\n",
    "                line = f\"\\\\multirow[c]{{{num_metrics}}}{{*}}{{{task_name}}} & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            else:\n",
    "                line = f\" & {metric_name} & {row_content} \\\\\\\\\"\n",
    "            latex_parts.append(line)\n",
    "        \n",
    "        if i < len(pivot_df.index.get_level_values('task').unique()) - 1:\n",
    "            latex_parts.append(f\"\\\\cmidrule{{1-{len(header_cols)}}}\")\n",
    "\n",
    "    latex_parts.extend([\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"])\n",
    "    return \"\\n\".join(latex_parts), df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76a7474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 26.54 & \\cellcolor{green!9}{28.07} & \\cellcolor{green!9}{28.07} \\\\\n",
      " & Acc Norm & 28.84 & \\cellcolor{green!6}{29.86} & \\cellcolor{green!3}{29.35} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 60.23 & \\cellcolor{green!21}{63.80} & \\cellcolor{green!18}{63.26} \\\\\n",
      " & Acc Norm & 56.52 & \\cellcolor{green!11}{58.38} & \\cellcolor{green!10}{58.29} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 35.52 & \\cellcolor{red!0}{35.38} & \\cellcolor{red!0}{35.43} \\\\\n",
      " & Acc Norm & 42.53 & \\cellcolor{green!1}{42.73} & \\cellcolor{green!6}{43.57} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 36.35 & \\cellcolor{red!6}{35.32} & \\cellcolor{green!4}{37.07} \\\\\n",
      " & Perplexity & 30.34 & \\cellcolor{red!60}{35.31} & \\cellcolor{green!60}{28.76} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 1.94 & \\cellcolor{green!2}{2.35} & \\cellcolor{green!1}{2.22} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 66.92 & \\cellcolor{red!3}{66.27} & \\cellcolor{green!3}{67.46} \\\\\n",
      " & Acc Norm & 66.65 & \\cellcolor{red!0}{66.49} & \\cellcolor{green!5}{67.57} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 83.20 & \\cellcolor{green!18}{86.30} & \\cellcolor{green!12}{85.30} \\\\\n",
      " & Acc Norm & 74.90 & \\cellcolor{green!15}{77.40} & \\cellcolor{green!29}{79.80} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & 39.82 & \\cellcolor{red!4}{39.00} & \\cellcolor{red!4}{39.00} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 4.93 & \\cellcolor{red!14}{2.55} & \\cellcolor{red!3}{4.37} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.86 & \\cellcolor{green!16}{0.88} & \\cellcolor{green!1}{0.86} \\\\\n",
      " & Byte Perplexity & 1.81 & \\cellcolor{red!20}{1.84} & \\cellcolor{red!1}{1.81} \\\\\n",
      " & Word Perplexity & 23.85 & \\cellcolor{red!60}{26.34} & \\cellcolor{red!60}{24.01} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "mtp_json_file = \"top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json\"\n",
    "vanilla_json_file = \"top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json\"\n",
    "top_json_file = \"top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json\"\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"TOP\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afde031a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "MTP    0.549709\n",
       "NTP    0.538886\n",
       "TOP    0.557166\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['metric'] == 'Acc Norm'].groupby('source')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8f9fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json', 'top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json', 'top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json', 'top_evals/zaydzuhri__vanilla-340M-4096-model/results_2025-08-20T11-23-06.323725.json', 'top_evals/zaydzuhri__mtp-340M-4096-model/results_2025-08-20T11-12-35.491684.json', 'top_evals/zaydzuhri__top-7B-4096-model/results_2025-08-20T10-46-34.638307.json', 'top_evals/zaydzuhri__myopic-340M-4096-model/results_2025-08-20T11-20-54.956874.json', 'top_evals/zaydzuhri__vanilla-7B-4096-model/results_2025-08-19T13-51-01.158842.json', 'top_evals/zaydzuhri__mtp-7B-4096-model/results_2025-08-20T10-59-16.724946.json']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "eval_logs_folder = \"top_evals\"\n",
    "all_json_files = glob(f\"{eval_logs_folder}/*/*.json\")\n",
    "print(all_json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b4fc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_evals/zaydzuhri__mtp-1.8B-4096-model/results_2025-08-20T12-31-13.727702.json\n",
      "top_evals/zaydzuhri__vanilla-1.8B-4096-model/results_2025-08-20T12-34-56.285949.json\n",
      "top_evals/zaydzuhri__myopic-1.8B-4096-model/results_2025-08-20T11-59-09.690564.json\n"
     ]
    }
   ],
   "source": [
    "print(mtp_json_file)\n",
    "print(vanilla_json_file)\n",
    "print(top_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6ebe055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & Top \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 35.58 & \\cellcolor{green!16}{38.40} & \\cellcolor{green!22}{39.25} \\\\\n",
      " & Acc Norm & 38.65 & \\cellcolor{green!11}{40.61} & \\cellcolor{green!22}{42.32} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 72.81 & \\cellcolor{red!0}{72.69} & \\cellcolor{green!4}{73.48} \\\\\n",
      " & Acc Norm & 67.05 & \\cellcolor{green!21}{70.66} & \\cellcolor{green!18}{70.12} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 46.03 & \\cellcolor{red!8}{44.61} & \\cellcolor{red!1}{45.75} \\\\\n",
      " & Acc Norm & 60.05 & \\cellcolor{red!10}{58.29} & \\cellcolor{green!2}{60.45} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 49.58 & \\cellcolor{red!9}{47.93} & \\cellcolor{green!4}{50.34} \\\\\n",
      " & Perplexity & 11.38 & \\cellcolor{red!60}{13.69} & \\cellcolor{green!60}{11.19} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 4.54 & \\cellcolor{red!0}{4.46} & \\cellcolor{green!4}{5.37} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 73.78 & \\cellcolor{red!9}{72.20} & \\cellcolor{red!3}{73.23} \\\\\n",
      " & Acc Norm & 73.50 & \\cellcolor{red!2}{73.07} & \\cellcolor{green!3}{74.16} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 90.20 & \\cellcolor{green!3}{90.70} & \\cellcolor{green!4}{91.00} \\\\\n",
      " & Acc Norm & 86.40 & \\cellcolor{green!4}{87.20} & \\cellcolor{green!9}{87.90} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & 41.56 & \\cellcolor{green!3}{42.12} & \\cellcolor{green!5}{42.53} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 11.85 & \\cellcolor{green!24}{15.98} & \\cellcolor{green!42}{18.93} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.73 & \\cellcolor{green!19}{0.76} & \\cellcolor{red!0}{0.73} \\\\\n",
      " & Byte Perplexity & 1.66 & \\cellcolor{red!22}{1.70} & \\cellcolor{green!0}{1.66} \\\\\n",
      " & Word Perplexity & 15.09 & \\cellcolor{red!60}{17.03} & \\cellcolor{green!31}{15.04} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "size = \"1.8B\"\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    if size in json_file:\n",
    "        if \"mtp\" in json_file:\n",
    "            mtp_json_file = json_file\n",
    "        elif \"vanilla\" in json_file:\n",
    "            vanilla_json_file = json_file\n",
    "        elif \"myopic\" in json_file or \"top\" in json_file:\n",
    "            top_json_file = json_file\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"Top\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b839ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source           task    metric     value\n",
      "1     NTP  Arc Challenge  Acc Norm  0.386519\n",
      "3     NTP       Arc Easy  Acc Norm  0.670455\n",
      "5     NTP      Hellaswag  Acc Norm  0.600478\n",
      "10    NTP           Piqa  Acc Norm  0.735038\n",
      "12    NTP           Sciq  Acc Norm  0.864000\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"metric\"] == \"Acc Norm\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b6c4b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "MTP    0.659666\n",
       "NTP    0.651298\n",
       "Top    0.669883\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['metric'] == 'Acc Norm'].groupby('source')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ffda1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Add this to your LaTeX preamble: \\usepackage[table]{xcolor} \\usepackage{multirow} \\usepackage{booktabs}\n",
      "\\begin{table}[htbp!]\n",
      "\\centering\n",
      "\\caption{Comparison of evaluation results. Colors relative to baseline.}\n",
      "\\label{tab:generated_comparison}\n",
      "\\begin{tabular}{l|l|rrr}\n",
      "\\toprule\n",
      "Task & Metric & NTP & MTP & TOP \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{Arc Challenge} & Acc & 45.05 & \\cellcolor{red!8}{43.69} & \\cellcolor{red!5}{44.20} \\\\\n",
      " & Acc Norm & 45.48 & \\cellcolor{green!0}{45.56} & \\cellcolor{green!5}{46.42} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Arc Easy} & Acc & 77.31 & \\cellcolor{green!2}{77.69} & \\cellcolor{green!4}{78.03} \\\\\n",
      " & Acc Norm & 74.07 & \\cellcolor{red!1}{73.86} & \\cellcolor{green!3}{74.62} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Hellaswag} & Acc & 50.95 & \\cellcolor{red!8}{49.58} & \\cellcolor{green!3}{51.53} \\\\\n",
      " & Acc Norm & 67.43 & \\cellcolor{red!9}{65.85} & \\cellcolor{green!7}{68.73} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Lambada} & Acc & 55.89 & \\cellcolor{red!16}{53.13} & \\cellcolor{green!6}{57.03} \\\\\n",
      " & Perplexity & 7.97 & \\cellcolor{red!60}{8.99} & \\cellcolor{green!60}{7.64} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Nq Open} & Exact Match & 7.31 & \\cellcolor{green!0}{7.40} & \\cellcolor{green!2}{7.70} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Piqa} & Acc & 76.33 & \\cellcolor{red!7}{75.08} & \\cellcolor{red!0}{76.17} \\\\\n",
      " & Acc Norm & 77.04 & \\cellcolor{red!7}{75.73} & \\cellcolor{red!3}{76.39} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{2}{*}{Sciq} & Acc & 92.90 & \\cellcolor{green!0}{93.00} & \\cellcolor{green!9}{94.50} \\\\\n",
      " & Acc Norm & 88.60 & \\cellcolor{green!4}{89.30} & \\cellcolor{green!18}{91.60} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Social Iqa} & Acc & --- & --- & --- \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{1}{*}{Triviaqa} & Exact Match & 24.28 & \\cellcolor{red!5}{23.36} & \\cellcolor{green!39}{30.90} \\\\\n",
      "\\cmidrule{1-5}\n",
      "\\multirow[c]{3}{*}{Wikitext} & Bits Per Byte & 0.66 & \\cellcolor{green!18}{0.69} & \\cellcolor{green!0}{0.66} \\\\\n",
      " & Byte Perplexity & 1.58 & \\cellcolor{red!20}{1.62} & \\cellcolor{red!0}{1.58} \\\\\n",
      " & Word Perplexity & 11.66 & \\cellcolor{red!60}{13.09} & \\cellcolor{red!1}{11.67} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "size = \"7B\"\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    if size in json_file:\n",
    "        if \"mtp\" in json_file:\n",
    "            mtp_json_file = json_file\n",
    "        elif \"vanilla\" in json_file:\n",
    "            vanilla_json_file = json_file\n",
    "        elif \"top\" in json_file or \"top\" in json_file:\n",
    "            top_json_file = json_file\n",
    "\n",
    "latex_output, df = generate_latex_comparison_table(\n",
    "    {\"NTP\" : vanilla_json_file, \n",
    "     \"MTP\" : mtp_json_file,\n",
    "     \"TOP\" : top_json_file}, \n",
    "     baseline_source_name=\"NTP\",\n",
    ")\n",
    "print(latex_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-plotting (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
